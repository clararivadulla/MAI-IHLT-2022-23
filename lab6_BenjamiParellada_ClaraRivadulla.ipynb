{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Jt3G2FGaGgwP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666881420065,"user_tz":-120,"elapsed":33219,"user":{"displayName":"Benjami Parellada Calderer","userId":"12702389916161093001"}},"outputId":"647429e1-7071-44d4-addf-059f34683f37"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# path where the data is contained\n","path = '/content/drive/MyDrive/IHLT/lab2/'\n","\n","import pandas as pd\n","import re\n","import nltk\n","from nltk.metrics import jaccard_distance\n","from nltk.corpus import wordnet\n","from nltk.stem import PorterStemmer\n","from nltk.stem import WordNetLemmatizer\n","from scipy.stats import pearsonr\n","from IPython.display import display_html \n","\n","nltk.download('omw-1.4')\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","nltk.download('averaged_perceptron_tagger')"]},{"cell_type":"markdown","source":["# Lab 6: Word Sense Disambiguation\n","\n","For the sixth practical of the subject, the goal is to get in touch with Lesk's algorithm. The statement is as follows:\n","\n","1. Read all pairs of sentences of the SMTeuroparl files of test set within the evaluation framework of the project.\n","2. Apply Lesk’s algorithm to the words in the sentences.\n","3. Compute their similarities by considering senses and Jaccard coefficient.\n","4. Compare the results with those in session 2 (document) and 3 (morphology) in which words and lemmas were considered.\n","5. Compare the results with gold standard by giving the Pearson correlation between them.\n","\n","\n","We will do three comparisons, first we check only those words that have representation in the WordNet with each other. However, since this comparison would not be totally fair, a second comparison removing the stopwords will also be done. Finally, to compare the method with the ones presented in the second and third sessions, we also add the tokens on the sets for those words that do not appear in the WordNet."],"metadata":{"id":"nRaXAO4EFARR"}},{"cell_type":"code","source":["# read the dataset and apply the lesk function for each pair of sentences\n","def data_reader(function_preprocess):\n","  dt = pd.read_csv(path + 'STS.input.SMTeuroparl.txt', sep='\\t', header = None) # i guess we could read this just once but eh\n","  dt[2] = dt.apply(lambda row: function_preprocess(row[0]), axis = 1)\n","  dt[3] = dt.apply(lambda row: function_preprocess(row[1]), axis = 1)\n","  dt['gs'] = pd.read_csv(path + 'STS.gs.SMTeuroparl.txt', sep='\\t', header = None)\n","  dt['jac'] = dt.apply(lambda row: 5*(1 - jaccard_distance(row[2], row[3])), axis = 1)\n","  #dt.drop_duplicates(subset = [0, 1], keep=False, inplace = True) # drop duplicate rows, but no one seems to be doing it so why should we \n","  return dt"],"metadata":{"id":"X45ya6DviZsB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Lesk\n","\n","In this basic case, we just get the POS tag of the sentence and use Lesk directly on the word-tag pair on the sentence. "],"metadata":{"id":"9I-r_GcLL2iz"}},{"cell_type":"code","source":["stopw = set(nltk.corpus.stopwords.words('english')) # english stopwords\n","wnl = WordNetLemmatizer() # initialize the lemmatizer\n","\n","# define the tags of the wordnet\n","tags = {'NN': wordnet.NOUN,\n","        'VB': wordnet.VERB,\n","        'JJ': wordnet.ADJ, \n","        'RB': wordnet.ADV}\n","\n","def lesk_basic(sentence):\n","  tokens = nltk.word_tokenize(sentence) # tokenize\n","  pairs = nltk.pos_tag(tokens) # get the pos of the tokens\n","  synsets = [nltk.wsd.lesk(sentence, pair[0], pos = tags.get(pair[1][:2].upper())) for pair in pairs] # use lesk on the sentence using the word and the tag\n","  synsets = [syn.name() for syn in synsets if syn] # filter empty results\n","  return set(synsets)"],"metadata":{"id":"2Gfb3uS2Gnul"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dt = data_reader(lesk_basic)\n","dt.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"YQFuef1hJUOB","executionInfo":{"status":"ok","timestamp":1666881425953,"user_tz":-120,"elapsed":4050,"user":{"displayName":"Benjami Parellada Calderer","userId":"12702389916161093001"}},"outputId":"80cf3a46-1b29-4101-dfe8-1940f22fb280"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                   0  \\\n","0  The leaders have now been given a new chance a...   \n","1  Amendment No 7 proposes certain changes in the...   \n","2  Let me remind you that our allies include ferv...   \n","3        The vote will take place today at 5.30 p.m.   \n","4  The fishermen are inactive, tired and disappoi...   \n","\n","                                                   1  \\\n","0  The leaders benefit aujourd' hui of a new luck...   \n","1  Amendment No 7 is proposing certain changes in...   \n","2  I would like to remind you that among our alli...   \n","3                 The vote will take place at 5.30pm   \n","4  The fishermen are inactive, tired and disappoi...   \n","\n","                                                   2  \\\n","0  {probability.n.01, give.v.43, seize.v.06, let....   \n","1  {amendment.n.02, propose.v.01, variety.n.06, s...   \n","2  {tax.n.01, maine.n.01, let.v.01, ally.n.01, in...   \n","3  {vote.n.04, today.n.02, astatine.n.01, take.v....   \n","4  {embody.v.02, tired.a.01, passive.a.01, fisher...   \n","\n","                                                   3    gs       jac  \n","0  {profit.v.01, therefore.r.01, seize.v.06, let....  4.50  1.875000  \n","1  {amendment.n.02, propose.v.01, variety.n.06, s...  5.00  4.500000  \n","2  {potent.a.03, wish.v.02, tax.n.01, embody.v.02...  4.25  1.250000  \n","3  {vote.n.04, astatine.n.01, take.v.39, topograp...  4.50  4.166667  \n","4  {embody.v.02, tired.a.01, passive.a.01, fisher...  5.00  5.000000  "],"text/html":["\n","  <div id=\"df-6c6e2276-443c-49a1-ae91-e6b378807245\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>gs</th>\n","      <th>jac</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>The leaders have now been given a new chance a...</td>\n","      <td>The leaders benefit aujourd' hui of a new luck...</td>\n","      <td>{probability.n.01, give.v.43, seize.v.06, let....</td>\n","      <td>{profit.v.01, therefore.r.01, seize.v.06, let....</td>\n","      <td>4.50</td>\n","      <td>1.875000</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Amendment No 7 proposes certain changes in the...</td>\n","      <td>Amendment No 7 is proposing certain changes in...</td>\n","      <td>{amendment.n.02, propose.v.01, variety.n.06, s...</td>\n","      <td>{amendment.n.02, propose.v.01, variety.n.06, s...</td>\n","      <td>5.00</td>\n","      <td>4.500000</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Let me remind you that our allies include ferv...</td>\n","      <td>I would like to remind you that among our alli...</td>\n","      <td>{tax.n.01, maine.n.01, let.v.01, ally.n.01, in...</td>\n","      <td>{potent.a.03, wish.v.02, tax.n.01, embody.v.02...</td>\n","      <td>4.25</td>\n","      <td>1.250000</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>The vote will take place today at 5.30 p.m.</td>\n","      <td>The vote will take place at 5.30pm</td>\n","      <td>{vote.n.04, today.n.02, astatine.n.01, take.v....</td>\n","      <td>{vote.n.04, astatine.n.01, take.v.39, topograp...</td>\n","      <td>4.50</td>\n","      <td>4.166667</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>The fishermen are inactive, tired and disappoi...</td>\n","      <td>The fishermen are inactive, tired and disappoi...</td>\n","      <td>{embody.v.02, tired.a.01, passive.a.01, fisher...</td>\n","      <td>{embody.v.02, tired.a.01, passive.a.01, fisher...</td>\n","      <td>5.00</td>\n","      <td>5.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6c6e2276-443c-49a1-ae91-e6b378807245')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-6c6e2276-443c-49a1-ae91-e6b378807245 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-6c6e2276-443c-49a1-ae91-e6b378807245');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["pearsonr(dt['gs'], dt['jac'])[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Gc3eegeAOmyQ","executionInfo":{"status":"ok","timestamp":1666881425954,"user_tz":-120,"elapsed":16,"user":{"displayName":"Benjami Parellada Calderer","userId":"12702389916161093001"}},"outputId":"345dcc3c-1fc0-40e7-9ab8-0810e51bde06"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.4823328683435585"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","source":["Seems to work pretty well, however, we are just using the words that have representation in the WordNet, hence this value is a bit inflated.\n","\n","Nevertheless, before we go on to unbias this, let us check the worst sentences to see where we could improve. We know that low Jaccard will mean that there are few words in the intersection of both sets, but let us see why. \n","\n","Looking at the output, we can see that the Lesk is returning quite a few very scientific synsets of the words. Specifically, one and two-letter words seems to think we are referring to chemical elements (`in` $\\rightarrow$ `indium.n.01`), while this is not the case. Most of these words are actually stopwords, and WordNet does not have a representation of these elements. Let us remove them before we proceed.\n","\n"],"metadata":{"id":"q2AbxfINZ9ow"}},{"cell_type":"code","source":["dt['diff'] = abs(dt['jac'] - dt['gs'])\n","dt_worst = dt.sort_values(by=['diff'], ascending=False).head(4)\n","df1_styler = dt_worst.style.set_table_attributes(\"style='display:inline'\").set_caption('Highest difference between Jaccard and Gold Standard')\n","display_html(df1_styler._repr_html_(), raw=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":213},"id":"Gn69dm9GZ4L-","executionInfo":{"status":"ok","timestamp":1666881425955,"user_tz":-120,"elapsed":15,"user":{"displayName":"Benjami Parellada Calderer","userId":"12702389916161093001"}},"outputId":"707088dc-d3d0-41d0-ef55-e704841f5a5b"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<style type=\"text/css\">\n","</style>\n","<table id=\"T_2b88f_\" style='display:inline'>\n","  <caption>Highest difference between Jaccard and Gold Standard</caption>\n","  <thead>\n","    <tr>\n","      <th class=\"blank level0\" >&nbsp;</th>\n","      <th class=\"col_heading level0 col0\" >0</th>\n","      <th class=\"col_heading level0 col1\" >1</th>\n","      <th class=\"col_heading level0 col2\" >2</th>\n","      <th class=\"col_heading level0 col3\" >3</th>\n","      <th class=\"col_heading level0 col4\" >gs</th>\n","      <th class=\"col_heading level0 col5\" >jac</th>\n","      <th class=\"col_heading level0 col6\" >diff</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th id=\"T_2b88f_level0_row0\" class=\"row_heading level0 row0\" >414</th>\n","      <td id=\"T_2b88f_row0_col0\" class=\"data row0 col0\" >Maij-Weggen report (A5-0323/2000)</td>\n","      <td id=\"T_2b88f_row0_col1\" class=\"data row0 col1\" >Report/ratio Maij-Weggen (A5-0323/2000)</td>\n","      <td id=\"T_2b88f_row0_col2\" class=\"data row0 col2\" >{'reputation.n.03'}</td>\n","      <td id=\"T_2b88f_row0_col3\" class=\"data row0 col3\" >set()</td>\n","      <td id=\"T_2b88f_row0_col4\" class=\"data row0 col4\" >4.750000</td>\n","      <td id=\"T_2b88f_row0_col5\" class=\"data row0 col5\" >0.000000</td>\n","      <td id=\"T_2b88f_row0_col6\" class=\"data row0 col6\" >4.750000</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_2b88f_level0_row1\" class=\"row_heading level0 row1\" >169</th>\n","      <td id=\"T_2b88f_row1_col0\" class=\"data row1 col0\" >The vote will take place today at 5.30 p.m.</td>\n","      <td id=\"T_2b88f_row1_col1\" class=\"data row1 col1\" >The vote will be to 17h30.</td>\n","      <td id=\"T_2b88f_row1_col2\" class=\"data row1 col2\" >{'vote.n.04', 'today.n.02', 'astatine.n.01', 'take.v.39', 'topographic_point.n.01', 'will.n.03'}</td>\n","      <td id=\"T_2b88f_row1_col3\" class=\"data row1 col3\" >{'will.v.02', 'exist.v.01', 'vote.n.05'}</td>\n","      <td id=\"T_2b88f_row1_col4\" class=\"data row1 col4\" >4.500000</td>\n","      <td id=\"T_2b88f_row1_col5\" class=\"data row1 col5\" >0.000000</td>\n","      <td id=\"T_2b88f_row1_col6\" class=\"data row1 col6\" >4.500000</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_2b88f_level0_row2\" class=\"row_heading level0 row2\" >36</th>\n","      <td id=\"T_2b88f_row2_col0\" class=\"data row2 col0\" >There must be a balance as a whole.</td>\n","      <td id=\"T_2b88f_row2_col1\" class=\"data row2 col1\" >Group must be in equilibrium.</td>\n","      <td id=\"T_2b88f_row2_col2\" class=\"data row2 col2\" >{'symmetry.n.01', 'whole.n.02', 'embody.v.02', 'there.n.01', 'deoxyadenosine_monophosphate.n.01', 'must.n.01'}</td>\n","      <td id=\"T_2b88f_row2_col3\" class=\"data row2 col3\" >{'mustiness.n.01', 'group.n.03', 'equilibrium.n.04', 'exist.v.01', 'indium.n.01'}</td>\n","      <td id=\"T_2b88f_row2_col4\" class=\"data row2 col4\" >4.500000</td>\n","      <td id=\"T_2b88f_row2_col5\" class=\"data row2 col5\" >0.000000</td>\n","      <td id=\"T_2b88f_row2_col6\" class=\"data row2 col6\" >4.500000</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_2b88f_level0_row3\" class=\"row_heading level0 row3\" >335</th>\n","      <td id=\"T_2b88f_row3_col0\" class=\"data row3 col0\" >Consumers will lose out, employees will lose out, Europe will lose competitive strength and growth.</td>\n","      <td id=\"T_2b88f_row3_col1\" class=\"data row3 col1\" >Users are the losers, with workers and European competitiveness and innovation régresseront.</td>\n","      <td id=\"T_2b88f_row3_col2\" class=\"data row3 col2\" >{'europe.n.01', 'suffer.v.11', 'increase.n.03', 'out.s.03', 'competitive.a.01', 'persuasiveness.n.01', 'consumer.n.01', 'will.n.03', 'employee.n.01'}</td>\n","      <td id=\"T_2b88f_row3_col3\" class=\"data row3 col3\" >{'invention.n.02', 'competitiveness.n.01', 'loser.n.03', 'embody.v.02', 'european.a.01', 'user.n.01', 'worker.n.03'}</td>\n","      <td id=\"T_2b88f_row3_col4\" class=\"data row3 col4\" >4.250000</td>\n","      <td id=\"T_2b88f_row3_col5\" class=\"data row3 col5\" >0.000000</td>\n","      <td id=\"T_2b88f_row3_col6\" class=\"data row3 col6\" >4.250000</td>\n","    </tr>\n","  </tbody>\n","</table>\n"]},"metadata":{}}]},{"cell_type":"markdown","source":["Let us do the same, however if the word in the POS tag is a stopword we skip it."],"metadata":{"id":"gYKcuuvh568P"}},{"cell_type":"code","source":["def lesk_stopless(sentence):\n","  tokens = nltk.word_tokenize(sentence) # tokenize\n","  pairs = nltk.pos_tag(tokens) # get the pos of the tokens\n","  synsets = [nltk.wsd.lesk(sentence, pair[0], pos = tags.get(pair[1][:2].upper())) for pair in pairs if pair[0].lower() not in stopw] # skip if the word is stopword else use lesk \n","  synsets = [syn.name() for syn in synsets if syn] # we ignore the words without meaning\n","  return set(synsets)"],"metadata":{"id":"tx6vfoc-jK_Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dt = data_reader(lesk_stopless)\n","dt.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"BCACTL3Pjqu6","executionInfo":{"status":"ok","timestamp":1666881427467,"user_tz":-120,"elapsed":1524,"user":{"displayName":"Benjami Parellada Calderer","userId":"12702389916161093001"}},"outputId":"be8a52e7-d462-4e61-869e-ffed1f0ef072"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                   0  \\\n","0  The leaders have now been given a new chance a...   \n","1  Amendment No 7 proposes certain changes in the...   \n","2  Let me remind you that our allies include ferv...   \n","3        The vote will take place today at 5.30 p.m.   \n","4  The fishermen are inactive, tired and disappoi...   \n","\n","                                                   1  \\\n","0  The leaders benefit aujourd' hui of a new luck...   \n","1  Amendment No 7 is proposing certain changes in...   \n","2  I would like to remind you that among our alli...   \n","3                 The vote will take place at 5.30pm   \n","4  The fishermen are inactive, tired and disappoi...   \n","\n","                                                   2  \\\n","0  {probability.n.01, give.v.43, seize.v.06, let....   \n","1  {amendment.n.02, propose.v.01, variety.n.06, s...   \n","2  {tax.n.01, let.v.01, ally.n.01, include.v.03, ...   \n","3  {take.v.39, vote.n.04, topographic_point.n.01,...   \n","4         {passive.a.01, tired.a.01, fisherman.n.01}   \n","\n","                                                   3    gs       jac  \n","0  {profit.v.01, therefore.r.01, seize.v.06, let....  4.50  1.818182  \n","1  {amendment.n.02, propose.v.01, variety.n.06, s...  5.00  5.000000  \n","2  {potent.a.03, wish.v.02, tax.n.01, ally.n.01, ...  4.25  1.875000  \n","3     {take.v.39, vote.n.04, topographic_point.n.01}  4.50  3.750000  \n","4         {passive.a.01, tired.a.01, fisherman.n.01}  5.00  5.000000  "],"text/html":["\n","  <div id=\"df-fa35f72e-c467-4c3e-875d-2699a978f2b5\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>gs</th>\n","      <th>jac</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>The leaders have now been given a new chance a...</td>\n","      <td>The leaders benefit aujourd' hui of a new luck...</td>\n","      <td>{probability.n.01, give.v.43, seize.v.06, let....</td>\n","      <td>{profit.v.01, therefore.r.01, seize.v.06, let....</td>\n","      <td>4.50</td>\n","      <td>1.818182</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Amendment No 7 proposes certain changes in the...</td>\n","      <td>Amendment No 7 is proposing certain changes in...</td>\n","      <td>{amendment.n.02, propose.v.01, variety.n.06, s...</td>\n","      <td>{amendment.n.02, propose.v.01, variety.n.06, s...</td>\n","      <td>5.00</td>\n","      <td>5.000000</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Let me remind you that our allies include ferv...</td>\n","      <td>I would like to remind you that among our alli...</td>\n","      <td>{tax.n.01, let.v.01, ally.n.01, include.v.03, ...</td>\n","      <td>{potent.a.03, wish.v.02, tax.n.01, ally.n.01, ...</td>\n","      <td>4.25</td>\n","      <td>1.875000</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>The vote will take place today at 5.30 p.m.</td>\n","      <td>The vote will take place at 5.30pm</td>\n","      <td>{take.v.39, vote.n.04, topographic_point.n.01,...</td>\n","      <td>{take.v.39, vote.n.04, topographic_point.n.01}</td>\n","      <td>4.50</td>\n","      <td>3.750000</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>The fishermen are inactive, tired and disappoi...</td>\n","      <td>The fishermen are inactive, tired and disappoi...</td>\n","      <td>{passive.a.01, tired.a.01, fisherman.n.01}</td>\n","      <td>{passive.a.01, tired.a.01, fisherman.n.01}</td>\n","      <td>5.00</td>\n","      <td>5.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fa35f72e-c467-4c3e-875d-2699a978f2b5')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-fa35f72e-c467-4c3e-875d-2699a978f2b5 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-fa35f72e-c467-4c3e-875d-2699a978f2b5');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["pearsonr(dt['gs'], dt['jac'])[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HSI2xcaejuij","executionInfo":{"status":"ok","timestamp":1666881427469,"user_tz":-120,"elapsed":13,"user":{"displayName":"Benjami Parellada Calderer","userId":"12702389916161093001"}},"outputId":"a1c2f5a6-0ca1-4436-e163-98ba7541cb07"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.5083583421535602"]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","source":["Analyzing the worst results, we can see that the first one is hopeless to solve without a smarter initial preprocessing. We could split every `/` we find into two different words, however that would also split the `A5-0323/2000`, which we may or may not want. \n","\n","The next worst result, is observation 169, where we can see that, even though we have the same word appearing, since it appears with different meanings in the sentence it is not considering it a match ('vote.n.04', and 'vote.n.05').\n","\n","For instance 36, we can see that since it must have read equilibrium, the definitions must have gone more mathematical. Making it choose the mathematical entry of the WordNet for group, making it then fail when choosing what entry to get for `must`. Still, even if we got must correct, this would not be such a great match since the other synsets are not matching either."],"metadata":{"id":"1_Z8SxN5nYjp"}},{"cell_type":"code","source":["dt['diff'] = abs(dt['jac'] - dt['gs'])\n","dt_worst = dt.sort_values(by=['diff'], ascending=False).head(4)\n","df1_styler = dt_worst.style.set_table_attributes(\"style='display:inline'\").set_caption('Highest difference between Jaccard and Gold Standard')\n","display_html(df1_styler._repr_html_(), raw=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":143},"id":"-tpbJkg2kzqK","executionInfo":{"status":"ok","timestamp":1666881427469,"user_tz":-120,"elapsed":10,"user":{"displayName":"Benjami Parellada Calderer","userId":"12702389916161093001"}},"outputId":"d67031e9-137a-435a-c6a4-cb2a0d59e2d4"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<style type=\"text/css\">\n","</style>\n","<table id=\"T_9398c_\" style='display:inline'>\n","  <caption>Highest difference between Jaccard and Gold Standard</caption>\n","  <thead>\n","    <tr>\n","      <th class=\"blank level0\" >&nbsp;</th>\n","      <th class=\"col_heading level0 col0\" >0</th>\n","      <th class=\"col_heading level0 col1\" >1</th>\n","      <th class=\"col_heading level0 col2\" >2</th>\n","      <th class=\"col_heading level0 col3\" >3</th>\n","      <th class=\"col_heading level0 col4\" >gs</th>\n","      <th class=\"col_heading level0 col5\" >jac</th>\n","      <th class=\"col_heading level0 col6\" >diff</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th id=\"T_9398c_level0_row0\" class=\"row_heading level0 row0\" >414</th>\n","      <td id=\"T_9398c_row0_col0\" class=\"data row0 col0\" >Maij-Weggen report (A5-0323/2000)</td>\n","      <td id=\"T_9398c_row0_col1\" class=\"data row0 col1\" >Report/ratio Maij-Weggen (A5-0323/2000)</td>\n","      <td id=\"T_9398c_row0_col2\" class=\"data row0 col2\" >{'reputation.n.03'}</td>\n","      <td id=\"T_9398c_row0_col3\" class=\"data row0 col3\" >set()</td>\n","      <td id=\"T_9398c_row0_col4\" class=\"data row0 col4\" >4.750000</td>\n","      <td id=\"T_9398c_row0_col5\" class=\"data row0 col5\" >0.000000</td>\n","      <td id=\"T_9398c_row0_col6\" class=\"data row0 col6\" >4.750000</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_9398c_level0_row1\" class=\"row_heading level0 row1\" >169</th>\n","      <td id=\"T_9398c_row1_col0\" class=\"data row1 col0\" >The vote will take place today at 5.30 p.m.</td>\n","      <td id=\"T_9398c_row1_col1\" class=\"data row1 col1\" >The vote will be to 17h30.</td>\n","      <td id=\"T_9398c_row1_col2\" class=\"data row1 col2\" >{'take.v.39', 'vote.n.04', 'topographic_point.n.01', 'today.n.02'}</td>\n","      <td id=\"T_9398c_row1_col3\" class=\"data row1 col3\" >{'vote.n.05'}</td>\n","      <td id=\"T_9398c_row1_col4\" class=\"data row1 col4\" >4.500000</td>\n","      <td id=\"T_9398c_row1_col5\" class=\"data row1 col5\" >0.000000</td>\n","      <td id=\"T_9398c_row1_col6\" class=\"data row1 col6\" >4.500000</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_9398c_level0_row2\" class=\"row_heading level0 row2\" >36</th>\n","      <td id=\"T_9398c_row2_col0\" class=\"data row2 col0\" >There must be a balance as a whole.</td>\n","      <td id=\"T_9398c_row2_col1\" class=\"data row2 col1\" >Group must be in equilibrium.</td>\n","      <td id=\"T_9398c_row2_col2\" class=\"data row2 col2\" >{'symmetry.n.01', 'must.n.01', 'whole.n.02'}</td>\n","      <td id=\"T_9398c_row2_col3\" class=\"data row2 col3\" >{'equilibrium.n.04', 'mustiness.n.01', 'group.n.03'}</td>\n","      <td id=\"T_9398c_row2_col4\" class=\"data row2 col4\" >4.500000</td>\n","      <td id=\"T_9398c_row2_col5\" class=\"data row2 col5\" >0.000000</td>\n","      <td id=\"T_9398c_row2_col6\" class=\"data row2 col6\" >4.500000</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_9398c_level0_row3\" class=\"row_heading level0 row3\" >31</th>\n","      <td id=\"T_9398c_row3_col0\" class=\"data row3 col0\" >Maij-Weggen report (A5-0323/2000)</td>\n","      <td id=\"T_9398c_row3_col1\" class=\"data row3 col1\" >Relation Maij-Weggen (A5-0323/2000)</td>\n","      <td id=\"T_9398c_row3_col2\" class=\"data row3 col2\" >{'reputation.n.03'}</td>\n","      <td id=\"T_9398c_row3_col3\" class=\"data row3 col3\" >{'sexual_intercourse.n.01'}</td>\n","      <td id=\"T_9398c_row3_col4\" class=\"data row3 col4\" >4.250000</td>\n","      <td id=\"T_9398c_row3_col5\" class=\"data row3 col5\" >0.000000</td>\n","      <td id=\"T_9398c_row3_col6\" class=\"data row3 col6\" >4.250000</td>\n","    </tr>\n","  </tbody>\n","</table>\n"]},"metadata":{}}]},{"cell_type":"markdown","source":["Now that we know this, let us add the tokens to make more comparable matches, since not all words appear in the WordNet."],"metadata":{"id":"-WfOqit454KD"}},{"cell_type":"code","source":["def lesk_sentence_fair(sentence, basic = True):\n","  tokens = nltk.word_tokenize(sentence) # tokenize\n","  pairs = nltk.pos_tag(tokens) # get the pos of the tokens\n","  pairs = [(re.sub(r'[^\\w\\s]', '', pair[0]), pair[1]) for pair in pairs]\n","  text = []\n","  for pair in pairs:\n","    if pair[0].lower() in stopw or re.match(r'^[_\\W]+$', pair[0].lower()) or not pair[0]: # if the token is a stopword or symbol we skip\n","      continue\n","    synset = nltk.wsd.lesk(sentence, pair[0], pos = tags.get(pair[1][:2].upper())) # use lesk on the sentence using the word and the tag\n","    if synset: # if the synset is not empty we just add it to the list\n","      text.append(synset.name())\n","    else: # else we add the token, there is no need to lemmatize because we need the WordNet representation of this. And since it failed in the above, it means there is none\n","      text.append(pair[0].lower())\n","  return set(text)"],"metadata":{"id":"-7VeKjArYVK3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dt = data_reader(lesk_sentence_fair)\n","df1_styler = dt.head().style.set_table_attributes(\"style='display:inline'\").set_caption('Highest difference between Jaccard and Gold Standard')\n","display_html(df1_styler._repr_html_(), raw=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":251},"id":"1kNlwrOzYW_F","executionInfo":{"status":"ok","timestamp":1666881676651,"user_tz":-120,"elapsed":1126,"user":{"displayName":"Benjami Parellada Calderer","userId":"12702389916161093001"}},"outputId":"976700c5-cff5-4261-8e6f-d1b6888d501d"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<style type=\"text/css\">\n","</style>\n","<table id=\"T_8fb6e_\" style='display:inline'>\n","  <caption>Highest difference between Jaccard and Gold Standard</caption>\n","  <thead>\n","    <tr>\n","      <th class=\"blank level0\" >&nbsp;</th>\n","      <th class=\"col_heading level0 col0\" >0</th>\n","      <th class=\"col_heading level0 col1\" >1</th>\n","      <th class=\"col_heading level0 col2\" >2</th>\n","      <th class=\"col_heading level0 col3\" >3</th>\n","      <th class=\"col_heading level0 col4\" >gs</th>\n","      <th class=\"col_heading level0 col5\" >jac</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th id=\"T_8fb6e_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n","      <td id=\"T_8fb6e_row0_col0\" class=\"data row0 col0\" >The leaders have now been given a new chance and let us hope they seize it.</td>\n","      <td id=\"T_8fb6e_row0_col1\" class=\"data row0 col1\" >The leaders benefit aujourd' hui of a new luck and let's let them therefore seize it.</td>\n","      <td id=\"T_8fb6e_row0_col2\" class=\"data row0 col2\" >{'probability.n.01', 'give.v.43', 'seize.v.06', 'let.v.01', 'new.a.06', 'hope.v.03', 'uranium.n.01', 'leadership.n.02'}</td>\n","      <td id=\"T_8fb6e_row0_col3\" class=\"data row0 col3\" >{'profit.v.01', 'therefore.r.01', 'seize.v.06', 'let.v.01', 'new.a.06', 'leadership.n.02', 'luck.n.03', 'hui', 'aujourd'}</td>\n","      <td id=\"T_8fb6e_row0_col4\" class=\"data row0 col4\" >4.500000</td>\n","      <td id=\"T_8fb6e_row0_col5\" class=\"data row0 col5\" >1.538462</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_8fb6e_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n","      <td id=\"T_8fb6e_row1_col0\" class=\"data row1 col0\" >Amendment No 7 proposes certain changes in the references to paragraphs.</td>\n","      <td id=\"T_8fb6e_row1_col1\" class=\"data row1 col1\" >Amendment No 7 is proposing certain changes in the references to paragraphs.</td>\n","      <td id=\"T_8fb6e_row1_col2\" class=\"data row1 col2\" >{'amendment.n.02', 'propose.v.01', 'variety.n.06', 'sealed.a.01', 'seven.s.01', 'paragraph.v.03', 'reference_book.n.01'}</td>\n","      <td id=\"T_8fb6e_row1_col3\" class=\"data row1 col3\" >{'amendment.n.02', 'propose.v.01', 'variety.n.06', 'sealed.a.01', 'seven.s.01', 'paragraph.v.03', 'reference_book.n.01'}</td>\n","      <td id=\"T_8fb6e_row1_col4\" class=\"data row1 col4\" >5.000000</td>\n","      <td id=\"T_8fb6e_row1_col5\" class=\"data row1 col5\" >5.000000</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_8fb6e_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n","      <td id=\"T_8fb6e_row2_col0\" class=\"data row2 col0\" >Let me remind you that our allies include fervent supporters of this tax.</td>\n","      <td id=\"T_8fb6e_row2_col1\" class=\"data row2 col1\" >I would like to remind you that among our allies, there are strong of this tax.</td>\n","      <td id=\"T_8fb6e_row2_col2\" class=\"data row2 col2\" >{'fervent', 'tax.n.01', 'let.v.01', 'ally.n.01', 'include.v.03', 'supporter.n.01', 'remind.v.01'}</td>\n","      <td id=\"T_8fb6e_row2_col3\" class=\"data row2 col3\" >{'potent.a.03', 'wish.v.02', 'tax.n.01', 'ally.n.01', 'would', 'among', 'remind.v.01'}</td>\n","      <td id=\"T_8fb6e_row2_col4\" class=\"data row2 col4\" >4.250000</td>\n","      <td id=\"T_8fb6e_row2_col5\" class=\"data row2 col5\" >1.363636</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_8fb6e_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n","      <td id=\"T_8fb6e_row3_col0\" class=\"data row3 col0\" >The vote will take place today at 5.30 p.m.</td>\n","      <td id=\"T_8fb6e_row3_col1\" class=\"data row3 col1\" >The vote will take place at 5.30pm</td>\n","      <td id=\"T_8fb6e_row3_col2\" class=\"data row3 col2\" >{'530', 'vote.n.04', 'today.n.02', 'take.v.39', 'topographic_point.n.01', 'promethium.n.01'}</td>\n","      <td id=\"T_8fb6e_row3_col3\" class=\"data row3 col3\" >{'take.v.39', 'vote.n.04', 'topographic_point.n.01', '530pm'}</td>\n","      <td id=\"T_8fb6e_row3_col4\" class=\"data row3 col4\" >4.500000</td>\n","      <td id=\"T_8fb6e_row3_col5\" class=\"data row3 col5\" >2.142857</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_8fb6e_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n","      <td id=\"T_8fb6e_row4_col0\" class=\"data row4 col0\" >The fishermen are inactive, tired and disappointed.</td>\n","      <td id=\"T_8fb6e_row4_col1\" class=\"data row4 col1\" >The fishermen are inactive, tired and disappointed.</td>\n","      <td id=\"T_8fb6e_row4_col2\" class=\"data row4 col2\" >{'passive.a.01', 'disappointed', 'tired.a.01', 'fisherman.n.01'}</td>\n","      <td id=\"T_8fb6e_row4_col3\" class=\"data row4 col3\" >{'passive.a.01', 'disappointed', 'tired.a.01', 'fisherman.n.01'}</td>\n","      <td id=\"T_8fb6e_row4_col4\" class=\"data row4 col4\" >5.000000</td>\n","      <td id=\"T_8fb6e_row4_col5\" class=\"data row4 col5\" >5.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n"]},"metadata":{}}]},{"cell_type":"code","source":["pearsonr(dt['gs'], dt['jac'])[0] "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a1LvBppTYY7G","executionInfo":{"status":"ok","timestamp":1666881681016,"user_tz":-120,"elapsed":405,"user":{"displayName":"Benjami Parellada Calderer","userId":"12702389916161093001"}},"outputId":"a4cba921-c8b7-40a5-979f-248ae969237c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.4780132616520053"]},"metadata":{},"execution_count":19}]},{"cell_type":"markdown","source":["We can see that adding the tokens helped some of the previous phrases, for example the ones we deemed hopeless, seem to have become better since the `A5-0323/2000` and the `Maij-Weggen` will be matched by only using tokens.\n","\n","However, we still have the same problem as before, with similar words getting different meaning depending on the exact position and the context of the sentence. We can now see `catastrophe.n.03` and `catastrophe.n.02` which in the first case it means a sudden violent change in the earths surface and in the second a state of extreme ruin. It seems that in the first sentence the use of version 03 seems wrong and should have gotten 02.\n","\n","For the tokens, there are still the problems we described in earlier sessions. For example, well as 530pm and 530 pm would still be considered different since in one case it is joined and in the other separated. More preprocessing to deal with these will be needed."],"metadata":{"id":"8lN0CU937M5p"}},{"cell_type":"code","source":["dt['diff'] = abs(dt['jac'] - dt['gs'])\n","dt_worst = dt.sort_values(by=['diff'], ascending=False).head(4)\n","df1_styler = dt_worst.style.set_table_attributes(\"style='display:inline'\").set_caption('Highest difference between Jaccard and Gold Standard')\n","display_html(df1_styler._repr_html_(), raw=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":213},"id":"58rU1uteriij","executionInfo":{"status":"ok","timestamp":1666881823163,"user_tz":-120,"elapsed":387,"user":{"displayName":"Benjami Parellada Calderer","userId":"12702389916161093001"}},"outputId":"b7fcc942-3d6f-4c54-f3c4-0be030843bf5"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<style type=\"text/css\">\n","</style>\n","<table id=\"T_bdcee_\" style='display:inline'>\n","  <caption>Highest difference between Jaccard and Gold Standard</caption>\n","  <thead>\n","    <tr>\n","      <th class=\"blank level0\" >&nbsp;</th>\n","      <th class=\"col_heading level0 col0\" >0</th>\n","      <th class=\"col_heading level0 col1\" >1</th>\n","      <th class=\"col_heading level0 col2\" >2</th>\n","      <th class=\"col_heading level0 col3\" >3</th>\n","      <th class=\"col_heading level0 col4\" >gs</th>\n","      <th class=\"col_heading level0 col5\" >jac</th>\n","      <th class=\"col_heading level0 col6\" >diff</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th id=\"T_bdcee_level0_row0\" class=\"row_heading level0 row0\" >169</th>\n","      <td id=\"T_bdcee_row0_col0\" class=\"data row0 col0\" >The vote will take place today at 5.30 p.m.</td>\n","      <td id=\"T_bdcee_row0_col1\" class=\"data row0 col1\" >The vote will be to 17h30.</td>\n","      <td id=\"T_bdcee_row0_col2\" class=\"data row0 col2\" >{'530', 'vote.n.04', 'today.n.02', 'take.v.39', 'topographic_point.n.01', 'promethium.n.01'}</td>\n","      <td id=\"T_bdcee_row0_col3\" class=\"data row0 col3\" >{'17h30', 'vote.n.05'}</td>\n","      <td id=\"T_bdcee_row0_col4\" class=\"data row0 col4\" >4.500000</td>\n","      <td id=\"T_bdcee_row0_col5\" class=\"data row0 col5\" >0.000000</td>\n","      <td id=\"T_bdcee_row0_col6\" class=\"data row0 col6\" >4.500000</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_bdcee_level0_row1\" class=\"row_heading level0 row1\" >36</th>\n","      <td id=\"T_bdcee_row1_col0\" class=\"data row1 col0\" >There must be a balance as a whole.</td>\n","      <td id=\"T_bdcee_row1_col1\" class=\"data row1 col1\" >Group must be in equilibrium.</td>\n","      <td id=\"T_bdcee_row1_col2\" class=\"data row1 col2\" >{'symmetry.n.01', 'must.n.01', 'whole.n.02'}</td>\n","      <td id=\"T_bdcee_row1_col3\" class=\"data row1 col3\" >{'equilibrium.n.04', 'mustiness.n.01', 'group.n.03'}</td>\n","      <td id=\"T_bdcee_row1_col4\" class=\"data row1 col4\" >4.500000</td>\n","      <td id=\"T_bdcee_row1_col5\" class=\"data row1 col5\" >0.000000</td>\n","      <td id=\"T_bdcee_row1_col6\" class=\"data row1 col6\" >4.500000</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_bdcee_level0_row2\" class=\"row_heading level0 row2\" >335</th>\n","      <td id=\"T_bdcee_row2_col0\" class=\"data row2 col0\" >Consumers will lose out, employees will lose out, Europe will lose competitive strength and growth.</td>\n","      <td id=\"T_bdcee_row2_col1\" class=\"data row2 col1\" >Users are the losers, with workers and European competitiveness and innovation régresseront.</td>\n","      <td id=\"T_bdcee_row2_col2\" class=\"data row2 col2\" >{'europe.n.01', 'suffer.v.11', 'increase.n.03', 'competitive.a.01', 'persuasiveness.n.01', 'consumer.n.01', 'employee.n.01'}</td>\n","      <td id=\"T_bdcee_row2_col3\" class=\"data row2 col3\" >{'invention.n.02', 'competitiveness.n.01', 'loser.n.03', 'régresseront', 'european.a.01', 'user.n.01', 'worker.n.03'}</td>\n","      <td id=\"T_bdcee_row2_col4\" class=\"data row2 col4\" >4.250000</td>\n","      <td id=\"T_bdcee_row2_col5\" class=\"data row2 col5\" >0.000000</td>\n","      <td id=\"T_bdcee_row2_col6\" class=\"data row2 col6\" >4.250000</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_bdcee_level0_row3\" class=\"row_heading level0 row3\" >452</th>\n","      <td id=\"T_bdcee_row3_col0\" class=\"data row3 col0\" >Then perhaps we could have avoided a catastrophe.</td>\n","      <td id=\"T_bdcee_row3_col1\" class=\"data row3 col1\" >We might have been able to prevent a disaster.</td>\n","      <td id=\"T_bdcee_row3_col2\" class=\"data row3 col2\" >{'catastrophe.n.03', 'could', 'possibly.r.01', 'keep_off.v.01'}</td>\n","      <td id=\"T_bdcee_row3_col3\" class=\"data row3 col3\" >{'able.a.01', 'prevent.v.02', 'catastrophe.n.02', 'might.n.01'}</td>\n","      <td id=\"T_bdcee_row3_col4\" class=\"data row3 col4\" >4.250000</td>\n","      <td id=\"T_bdcee_row3_col5\" class=\"data row3 col5\" >0.000000</td>\n","      <td id=\"T_bdcee_row3_col6\" class=\"data row3 col6\" >4.250000</td>\n","    </tr>\n","  </tbody>\n","</table>\n"]},"metadata":{}}]},{"cell_type":"markdown","source":["\n","# Conclusion\n","\n","We have compared three different ways of doing the STS task using Word Sense Disambiguation using the classic Lesk algorithm. In the first case, just using the given sentence and extracting the POS to identify the word, we see that it returns adequate results, however in closer inspection we see that some words matched should not really be matched. For example, we saw that not removing the stopwords, since these are not represented in the WordNet, will return us incorrect synsets about the word. Moreover, since these words are usually 2 letters long, it confuses them with words on the periodic table.\n","\n","The second Lesk implementation, fixed this by first checking if the word is a stopword and omitting the word if it was when using Lesk. We feel this is a more realistic approach to dealing with the words, since it is what we also did on the Lab 2 and Lab 3. Hence, it will be better for comparison as well. We can see that removing the stopwords gives us higher correlation to the gold standard, as the unexpected synsets we get from the stopwords are now removed.\n","\n","| Processing | Pearson $r$ |  \n","|------------|:---:|\n","| Lesk     | 0.482 |\n","| Lesk + Stopwords   | 0.508  |\n","\n","\n","Finally, since in these previous two implementations we only have compared the words that appeared in the WordNet with themselves, we decided to add those words that did not appear as their token. This will make the comparison between the previous lab sessions better, since otherwise we are inflating the results by not using all the text that appears (numbers, given names, etc.). We do not need to lemmatize these words, since lemmatizing also uses the WordNet, and if Lesk returns us that there is nothing, then there will be nothing to lemmatize as well. We also remove the stopwords and tokens that only contain non-alphanumeric symbols, to match the preprocessing of the previous sessions. Furthermore, we do not remove the symbols inside the synset representations (e.g. `vote.n.03` stays the same). With these restrictions, the resulting Pearson Correlations are given in the following table.\n","\n","| Processing | Pearson $r$ |  \n","|------------|:---:|\n","| Simple (Lab 2)    | 0.468 |\n","| Stemming (Lab 2)  | 0.462  |\n","| Lemmatizing (Lab 3) | 0.483  |\n","| **Lesk + Stopwords + Tokens** | **0.478**  |\n","\n","As we can see in the table, the use of Lesk returns pretty similar results to the previous Labs, better than just using the tokens directly as well as using Stemming. However, when compared to Lemmatizing, we seem to be obataining a bit worse results.\n","\n","Lesk basically gets the maximum intersection of the `context` sentence with the definitions of the synsets found from the ambiguous word, making it very sensitive to the exact wording of definitions. Hence, the absence of certain words can radically change the results. Moreover, it only checks overlaps among the glosses of the senses being considered, and these are usually not long enough to provide a good enough distinction.\n","\n","This can be seen clearly in some of the previous examples. Where, for instance, the word `vote` appeared and the result of the Lesk algorithm was two different results in significance:\n","\n","\n","- `The vote will take place today at 5.30 p.m.` $\\rightarrow$ `vote.n.04`\n","- `The vote will be to 17h30.`  $\\rightarrow$ `vote.n.05` (is this even a correct sentence?)\n","\n","Which makes our naive comparison fail, since we are checking if the whole word is a match. This could be fixed by just omitting the word function and entry used (everything after the point in the synset name). However, we were told by a higher power, our savior, Salvador, to not do it.\n","\n","Overall, the WDS seems a better approach to solving the STS problem, since it makes more sense to use the words and find their possible synset. Nevertheless, there are still some quirks to fix since some of these words are obtaining the wrong entry in the WordNet, making it fail the set intersection."],"metadata":{"id":"ZISPHCWzFH0V"}},{"cell_type":"code","source":[],"metadata":{"id":"lXhg9BH45Dl0"},"execution_count":null,"outputs":[]}]}