{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mWuWfLra_YOC"
   },
   "source": [
    "# ELIMINAR AQUEST CHUNK\n",
    "# !!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "\n",
    "Lexical features, semantic features and combination. Use all \n",
    "new components: such as ML, or other options from the table\n",
    "\n",
    "\n",
    "Rubric:\n",
    "\n",
    "\t- Bad approach: Not using feature selection -> is penalizable using hand crafted approaches\n",
    "\t\t  - ((Using test set for anything, especially for selecting the model))\n",
    "\t- Explain why we use a subset of the features\n",
    "\t- Lexical features, semantic features and combination. Show results at least in feature selection.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ldADwroS_fds",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-14 17:53:28.401832: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-14 17:53:28.462302: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-12-14 17:53:28.827506: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-14 17:53:28.827538: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-14 17:53:28.827541: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2022-12-14 17:53:29.194978: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_SYSTEM_DRIVER_MISMATCH: system has unsupported display driver / cuda driver combination\n",
      "2022-12-14 17:53:29.194996: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: artixdesk\n",
      "2022-12-14 17:53:29.195002: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: artixdesk\n",
      "2022-12-14 17:53:29.195083: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 525.60.11\n",
      "2022-12-14 17:53:29.195091: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.56.6\n",
      "2022-12-14 17:53:29.195092: E tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:313] kernel version 520.56.6 does not match DSO version 525.60.11 -- cannot find working devices in this configuration\n",
      "[nltk_data] Downloading package words to /home/beny/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/beny/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet_ic to /home/beny/nltk_data...\n",
      "[nltk_data]   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/beny/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/beny/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/beny/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/beny/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/beny/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package conll2000 to /home/beny/nltk_data...\n",
      "[nltk_data]   Package conll2000 is already up-to-date!\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'neuralcoref' has no attribute 'add_to_pipe'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcode_\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_reader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data_reader\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcode_\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extractor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Features\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcode_\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Models\n",
      "File \u001b[0;32m~/Documents/MAI-projects/IHLT/STS/code_/feature_extractor.py:41\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[39m# initialize spacy and neuralcoref\u001b[39;00m\n\u001b[1;32m     40\u001b[0m nlp \u001b[39m=\u001b[39m spacy\u001b[39m.\u001b[39mload(\u001b[39m'\u001b[39m\u001b[39men_core_web_sm\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 41\u001b[0m neuralcoref\u001b[39m.\u001b[39;49madd_to_pipe(nlp)\n\u001b[1;32m     43\u001b[0m \u001b[39m# set corpus\u001b[39;00m\n\u001b[1;32m     44\u001b[0m semcor_ic \u001b[39m=\u001b[39m wordnet_ic\u001b[39m.\u001b[39mic(\u001b[39m'\u001b[39m\u001b[39mic-semcor.dat\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'neuralcoref' has no attribute 'add_to_pipe'"
     ]
    }
   ],
   "source": [
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    from code_.data_reader import data_reader\n",
    "from code_.feature_extractor import Features\n",
    "from code_.model import Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q9c2S5h33Z5E"
   },
   "source": [
    "# Semantic Textual Similarity\n",
    "\n",
    "Authors:\n",
    "\n",
    "    - Benjami Parellada\n",
    "    - Clara Rivadulla\n",
    "\n",
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KoRr0_LGrEv9"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "Semantic Textual Similarity (STS) measures the degree of semantic equivalence between two texts. It is also known as paraphrase detection, where a pair of texts is a paraphrase when both texts describe the same meaning with different words. Before the advent of neural networks and word embeddings with `word2vec` by Google in 2013 - where an \"embedding\" is a numeric vector representation of natural language texts such that computers can understand the context and meaning - this was a difficult problem to solve. The nuances of natural language make it hard for machines to understand the context and meaning of different texts, for example, two sentences could have no word in common but still mean the same. STS is related to numerous NLP tasks, such as machine translation, text summarization, machine reading and understanding, question answering, among other tasks. \n",
    "\n",
    "Thus, in this project, we will travel back in time, to an era before word embeddings reigned supreme, to understand and extract different features from text in order to compare the similarity between two sentences. Concretely, we use the data set and description of task Semantic Textual Similarity from SemEval-2012 Task 6. Using this data, we extract lexical and syntactical dimensions in order to train a model that is able to detect when two sentences are similar. Moreover, we will comment and compare the different features in this scenario explaining which are the most relevant.\n",
    "\n",
    "## Structure of the project\n",
    "\n",
    "The whole project is contained in a general folder, `STS`, which contains the following directories with their respective files:\n",
    "- `code_`: where the `.py` files used for reading the data (`data_reader.py`), pre-processing it, extracting the desired features (`feature_extractor.py`) and defining the models (`model.py`) to be trained are.\n",
    "- `test-gold` and `train`: where test and training data is (`'SMTeuroparl'` `.txt` files).\n",
    "\n",
    "## Dataset\n",
    "\n",
    "The source of the dataset is from *SemEval-2012 Task 6*, where 3 different datasets have from different sources have been manually tagged. The three sources of data all have the same format, where there are two sentences $S_1$ and $S_2$ and the objective is to compute how similar these sentences are to each other. The datasets are:\n",
    "\n",
    "- MSR-Paraphrase, Microsoft Research Paraphrase Corpus, which contains 750 pairs of sentences about thousands of news sources from the web.\n",
    "- MSR-Video, Microsoft Research Video Description Corpus, which contains 750 pairs of sentences of sentences describing a video.\n",
    "- SMTeuroparl: WMT2008 develoment dataset (Europarl section), which contains 734 pairs of sentences from a translation from French to English.\n",
    "\n",
    "These three datasets are given to us to train and do model selection. Finally, there are more 5 more datasets which were not presented that contain the evaluation test partition of the data. The following summarizes these:\n",
    "\n",
    "- For MSR-Paraphrase, we are given 750 unseen pairs of sentences, where the final model should be evaluated.\n",
    "- For MSR-Video they add 750 unseen pairs of sentences.\n",
    "- For SMTeuroparl, they add 459 pairs.\n",
    "\n",
    "Additionally, two extra datasets are provided which are from different context.\n",
    "- SMTnews, which contains 399 pairs of sentences from news conversation sentence pairs from WMT.\n",
    "- OnWN, which contains 750 pairs of sentences where the first comes from Ontonotes and the second from a WordNet definition.\n",
    "\n",
    "We explain these to understand the context of the task, however, we will not be using any external information that could bias our Test evaluation metrics. We will only train the machine learning models, do feature selection, and model selection, using only the original train data given.\n",
    "\n",
    "## System Evaluation\n",
    "\n",
    "As already mentioned, the target feature we are trying to predict is how similar two sentences are. Given two sentences $S_1$ and $S_2$, we are trying the similarity score. This performance is evaluated using the Pearson product-moment correlation coefficient between the output score from our system and the human score, henceforth, referred as Gold Standard. Reading through the overall conclusion paper on the SemEval-2012 Task 6, we see that there is some controversy, as in it they concatenate the results of all the datasets before calculating the Pearson correlation. This is said to lose some of the individual scores on the datasets. Nevertheless, for the purpose of this project, we will do the concatenation of all the datasets into one to train and evaluate. We feel it makes more sense to have a global dataset that tries to predict how similar two sentences are independent of the original context.\n",
    "\n",
    "Regarding the Gold Standard, it was annotated by humans where they were asked to score the pairs with the following scale interpretation:\n",
    "\n",
    "0. The two sentences are on different topics.\n",
    "1. The two sentences are not equivalent, but are on the same topic.\n",
    "2. The two sentences are not equivalent, but share some details.\n",
    "3. The two sentences are roughly equivalent, but some important information differs/missing.\n",
    "4. The two sentences are mostly equivalent, but some unimportant details differ.\n",
    "5. The two sentences are completely equivalent, as they mean the same thing.  \n",
    "\n",
    "For each sentence pair, the Gold Standard represents the average of 5 scores from different annotators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wE97phyG3Z5E",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Methodology\n",
    "After this not-so-short introduction, we present the work we have done in order to complete the STS task. The sections we cover are:\n",
    "- Preprocessing.\n",
    "- Feature Extraction.\n",
    "- Feature Importance and selection.\n",
    "- Model Building.\n",
    "- Model Selection.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "curuQez9-cjg"
   },
   "source": [
    "\n",
    "## Preprocessing\n",
    "\n",
    "In order to do the preprocessing of the sentences, we perform the following transformations with the `preprocess` function in `feature_extractor.py` to every sentence:\n",
    "- Convert it to **ASCII** with the `unidecode` function.\n",
    "- Remove hyphens and forward slashes.\n",
    "- Angular brackets (`<`, `>`) that enclose the tokens are stripped (matches the interior string of the format `<XYZ>` and returns `XYZ`).\n",
    "- Normalize dollar (`$`) values, as no other monetary symbol appears in the train dataset.\n",
    "- Convert `n't` to `not`, `'t` to `not`, `'re` to `are`, `'s` to `is`, `'d` to `would`, `'ll` to `will`, `'ve` to `have`, `'m` to `am`, and double blankspaces for single ones.\n",
    "\n",
    "We also apply a preprocessing function, `preprocess_tokenized`, to the tokens of every sentece in order to round numeric values to just one decimal or convert them to integer in case they finish with `'.0'`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WS2ITwJv-gZD"
   },
   "source": [
    "## Feature extraction\n",
    "Before we can train any model, we need to extract features from the sentence pairs. We have explored many different features, here is a little explanation of each and every one:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p91S-pna-k0m"
   },
   "source": [
    "### Lexical Features\n",
    "\n",
    "For every lexical feature, we keep them both **with and without stop-words**. \n",
    "\n",
    "##### **Tokens**\n",
    "We extract tokens and their POS-tags with `nltk`'s `word_tokenize` and `pos_tag` functions respectively. We apply the mentioned `preprocess_tokenized` function to the extracted tokens.\n",
    "\n",
    "##### **Lemmas**\n",
    "We extract lemmas from tokens with the implemented `pos_wn` function and with the use of the `WordNetLemmatizer`. We also extract lemmas using `spacy` with the `en_core_web_sm` model. \n",
    "\n",
    "##### **Word N-grams**\n",
    "We use `nltk`'s `ngrams` function to get *unigrams*, *bigrams*, *trigrams* and *fourgrams* of words from the tokens of every sentence.\n",
    "\n",
    "##### **Character N-grams**\n",
    "We use `nltk`'s `ngrams` function to get *bigrams*, *trigrams* and *fourgrams* of characters from the tokens of every sentence, with and without stop-words. \n",
    "\n",
    "##### **Longest common subsequence**\n",
    "We calculate the longest common subsequence between every two sentences from the list of tokens of each one and normalize it dividing it by the length of the shortest sentence. \n",
    "\n",
    "##### **Longest common substring**\n",
    "The same as the previous one, but calculating the longest common substring instead. \n",
    "\n",
    "##### **Levenshtein distance**\n",
    "We use `nltk`'s `edit_distance` function to compute the Levenshtein distance (minimum number of single-character edits (insertions, deletions or substitutions) required to change one sentence into the other) between the two sentences (considering a sentence as its lemmas joined by a blankspace). We normalize the output dividing it by the length of the longest sentence. `nltk.edit_distance(sent1, sent2) / max(len(sent1), len(sent2))`. \n",
    "\n",
    "##### **Function Word Similarity**\n",
    "We use **function word frequency vectors** to compare **stopwords**' frequencies among pairs of sentences. That is, we create two vectors of 0s of length equal to the number of stopwords and put 1 if a stopword is among the sentence's tokens. Once we've iterated over all the stopwords, we return the correlation between these two vectors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2vinfcrc-mm7"
   },
   "source": [
    "### Syntactical Features\n",
    "\n",
    "##### **Lesk**\n",
    "We also extract synsets using WordNet's lesk \n",
    "\n",
    "##### **Synsets Similarities**\n",
    "We use `nltk`'s synset similarity functions to compare the most frequent synsets of every token in a sentence to the most frequent synsets of every token in the other one. The similarities we compute for every pair of sentences are:  **Leacock-Chodorow similarity**, **Path similarity**, **Wu-Palmer similarity**, and **Lin similarity**. The resulting feature is the sum of these similarities divided by the length of the similarities list between the most frequent synsets of the tokens of the two sentences.\n",
    "\n",
    "\n",
    "##### **Named Entities**\n",
    "We chunk POS pairs with `nltk`'s `ne_chunk` function, then get the triads with the token, POS tag, and if it's a named entity with the `tree2conlltags` function. We discard stop-words, empty or non-alphanumeric token and keep only named entities and other tokens. \n",
    "\n",
    "##### **Vector space sentence**\n",
    "Each sentence is represented as a single distributional vector $u(·)$ by summing the distributional vector of each word w in the sentence $S: u(S) = ∑_{w∈S} x_w$, where $x_w$ is the vector representation of the word $w$. The vector space sentence similarity is: $|cos(u(S_1), u(S_2))|$\n",
    "\n",
    "##### **Vector space sentence IC** \n",
    "Similar to the previous representation, $u_W(·)$ uses the information content $ic(w)$ to weigh the LSA vector of each word before summation: $u_w(S) = ∑_{w∈S} ic(w)x_w$. The vector space sentence similarity is: $|cos(u_w(S_1), u_w(S_2))|$\n",
    "\n",
    "##### **Wordnet augmented word overlap**\n",
    "For this, WordNet is used to assign partial scores to words that are not common to both sentences. The WordNet augmented coverage $P_{WN} (·, ·)$ is defined as:\n",
    "\n",
    "$$P_{WN} (S_1, S_2) =  \\frac{1}{|S_2|} \\sum_{w_1∈S_1}score(w_1, S_2)$$\n",
    "\n",
    "$$score(w,S)=\\begin{cases}\n",
    "1 \\quad &\\text{if} \\ w∈S  \\newline max_{w'∈S} sim(w, w') \\quad &\\text{otherwise}  \\\\\n",
    "     \\end{cases}$$\n",
    "\n",
    "where $sim(·, ·)$ represents the WordNet path length similarity. The WordNet-augmented word overlap feature is defined as a harmonic mean of $P_{WN} (S_1, S_2)$ and $P_{WN} (S_2, S_1)$.\n",
    "\n",
    "##### **Weighted word overlap**\n",
    "We use the information content $ic(w)=ln\\frac{∑_{w'∈C}freq(w')}{freq(w)}$, where $C$ is the set of words in the corpus and $freq(w)$ is the frequency of the word $w$ in the corpus. The weighted word coverage of the second sentence by the first sentence is given by: $wwc(S_1, S_2)=\\frac{∑_{w∈S_1∩S_2}ic(w)}{∑_{w'∈S_2}ic(w')}$. The weighted word overlap between two sentences is calculated as the harmonic mean of the $wwc(S_1, S_2)$ and $wwc(S_2, S_1)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ufhN2wOt-oCs"
   },
   "source": [
    "\n",
    "### Other Features\n",
    "\n",
    "##### **Number of POS tags**\n",
    "We extract a new feature that compares some types of POS tags between the 2 sentences. If both sentences contain no tag of the selected type or if they have the same count, 1 is returned. Otherwise, we return $1-\\frac{|count_1 - count_2|}{count_1 + count_2}$, where $count_1$ and $count_2$ are the number of POS tag of that type in each sentence. We do that for:\n",
    "\n",
    "1. **Verbs**\n",
    "2. **Nouns**\n",
    "3. **Adjectives**\n",
    "4. **Adverbs**\n",
    "\n",
    "##### **Numeric features**\n",
    "The following features compare the sets of numbers $N_1$ and $N_2$ in two sentences:\n",
    "1. **Numeric Feature: Log**: $log (1 + |N_1| + |N_2|)$\n",
    "2. **Numeric Feature: Intersection**: $2\\frac{|N_1 ∩ N_2|}{(|N_1| + |N_2|)}$\n",
    "3. **Numeric Feature: Bool**: $N_1 = N_2$,\n",
    "$N_1∩N_2 \\neq ∅$, and $N_1 ⊆ N_2∨N_2 ⊆ N_1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yGRghc1p-pzd"
   },
   "source": [
    "### Similarities\n",
    "##### **Jaccard**\n",
    "We compute Jaccard similarity for each pair of sentences using *tokens*, *lemmas*, *n-grams* and *lesk*. \n",
    "The `nltk` function used is `jaccard_distance` and it gives us the **distance** between both sentences, while we are interested in the **similarity**. By using the distance, when two sets are similar we get a value closer to 0, meanwhile when they are more distinct we get a value closer to 1. This is different from how the gold standard is in the dataset, similar sentences have higher values, and dissimilar sentences have lower values.\n",
    "Hence, this is why we use the similarity, which is the inverse of the distance, and we can calculate it as such:\n",
    "\n",
    "$$similarity_{jaccard} = 1 - distance_{jaccard}$$\n",
    "\n",
    "##### **Dice**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ty4cDHVT3Z5E",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Read the data\n",
    "x_train, y_train = data_reader(train = True, input_files = ['MSRpar.txt', 'MSRvid.txt', 'SMTeuroparl.txt'])\n",
    "x_test, y_test = data_reader(train = False, input_files = ['MSRpar.txt', 'MSRvid.txt', 'SMTeuroparl.txt', 'surprise.OnWN.txt', 'surprise.SMTnews.txt'])\n",
    "# Compute the features\n",
    "train_features = Features(x_train)\n",
    "test_features = Features(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4058434978.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn [2], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    mkdir temp\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "mkdir temp\n",
    "\n",
    "cd temp\n",
    "\n",
    "!git clone https://github.com/huggingface/neuralcoref.git\n",
    "!pip install -U spacy\n",
    "!python -m spacy download en\n",
    "\n",
    "cd neuralcoref\n",
    "\n",
    "!pip install -r requirements.txt\n",
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-14 18:37:27.905599: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-14 18:37:27.963276: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-12-14 18:37:28.330256: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-14 18:37:28.330288: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-14 18:37:28.330291: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2022-12-14 18:37:28.744848: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_SYSTEM_DRIVER_MISMATCH: system has unsupported display driver / cuda driver combination\n",
      "2022-12-14 18:37:28.744864: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: artixdesk\n",
      "2022-12-14 18:37:28.744866: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: artixdesk\n",
      "2022-12-14 18:37:28.744934: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 525.60.11\n",
      "2022-12-14 18:37:28.744942: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.56.6\n",
      "2022-12-14 18:37:28.744944: E tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:313] kernel version 520.56.6 does not match DSO version 525.60.11 -- cannot find working devices in this configuration\n",
      "[nltk_data] Downloading package words to /home/beny/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/beny/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet_ic to /home/beny/nltk_data...\n",
      "[nltk_data]   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/beny/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/beny/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/beny/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/beny/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/beny/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package conll2000 to /home/beny/nltk_data...\n",
      "[nltk_data]   Package conll2000 is already up-to-date!\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'neuralcoref' has no attribute 'add_to_pipe'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata.pickle\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m----> 3\u001b[0m     train_features \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m      4\u001b[0m     test_features \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n",
      "File \u001b[0;32m~/Documents/MAI-projects/IHLT/STS/code_/feature_extractor.py:41\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# initialize spacy and neuralcoref\u001b[39;00m\n\u001b[1;32m     40\u001b[0m nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 41\u001b[0m \u001b[43mneuralcoref\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_to_pipe\u001b[49m(nlp)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# set corpus\u001b[39;00m\n\u001b[1;32m     44\u001b[0m semcor_ic \u001b[38;5;241m=\u001b[39m wordnet_ic\u001b[38;5;241m.\u001b[39mic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mic-semcor.dat\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'neuralcoref' has no attribute 'add_to_pipe'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('data.pickle', 'rb') as f:\n",
    "    train_features = pickle.load(f)\n",
    "    test_features = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eI1UiK3f3Z5G",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Modeling\n",
    "\n",
    "In this section, we explain how we combine the features ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9nIL4Mw49q5J"
   },
   "source": [
    "### Machine Learning\n",
    "\n",
    "...\n",
    "\n",
    "#### SVM\n",
    "\n",
    "...\n",
    "\n",
    "#### Random Forest\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2FI72DFfAwvu"
   },
   "source": [
    "### Frameworks \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "St6GB9pJA282"
   },
   "source": [
    "#### Simple\n",
    "\n",
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2VPmeSxA5Q7"
   },
   "source": [
    "\n",
    "#### Ensemble\n",
    "\n",
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fMh_taDhA6kO"
   },
   "source": [
    "#### Ensemble Combined\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WNNmvkzZ-FfM"
   },
   "source": [
    "### Feature Selection\n",
    "\n",
    "...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bhVYtnLgBUKb"
   },
   "source": [
    "### Model Selection\n",
    "\n",
    "We grid search using CV, select the one that has better Pearson on train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rJu_EJ628p3O"
   },
   "source": [
    "# Results\n",
    "\n",
    "Finally, we present the results of the project. In this section, the afromentioned topics are executed and we explain the importance of the variables as well as the Pearson correlation obtained.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AG-Yz2GmB615"
   },
   "source": [
    "## Feature Exploration\n",
    "\n",
    "We first do a correlation matrix to observe the importance of the features.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
