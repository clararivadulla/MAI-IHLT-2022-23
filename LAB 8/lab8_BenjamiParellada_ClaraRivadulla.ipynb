{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q3WehPiEa8M-",
    "outputId": "e2c55a9a-b518-4fea-95a7-cf3eec20d2d3"
   },
   "outputs": [],
   "source": [
    "path = 'lab8/'\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import os\n",
    "from nltk import CFG, BottomUpChartParser, BottomUpLeftCornerChartParser, LeftCornerChartParser\n",
    "from nltk.metrics import jaccard_distance\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from scipy.stats import pearsonr\n",
    "from IPython.display import display_html\n",
    "import svgling\n",
    "import contextlib\n",
    "\n",
    "from nltk.parse.corenlp import CoreNLPDependencyParser\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QMuuOQ1i7qzP"
   },
   "source": [
    "# Lab 8: Parsing\n",
    "\n",
    "For the eighth practical of the subject, the goal is to try some non-probabilistic parsers, and optionally probabilistic parsers as well. The **mandatory** statement is:\n",
    "\n",
    "1. Consider the following sentence:\n",
    "`Lazy cats play with mice.`\n",
    "2. Expand the grammar of the example related to non-probabilistic chart parsers in order to subsume this new sentence.\n",
    "3. Perform the constituency parsing using a BottomUpChartParser, a BottomUpLeftCornerChartParser and a LeftCornerChartParser.\n",
    "4. For each one of them, provide the resulting tree, the number of edges and the list of explored edges.\n",
    "5. Which parser is the most efficient for parsing the sentence?\n",
    "6. Which edges are filtered out by each parser and why?\n",
    "\n",
    "The **optional** statement, which we've also accomplished, is:\n",
    "\n",
    "1. Read all pairs of sentences of the SMTeuroparl files of test set within the evaluation framework of the project.\n",
    "2. Compute the Jaccard similarity of each pair using the dependency triples from CoreNLPDependencyParser.\n",
    "3. Show the results. Do you think it could be relevant to use NEs to compute the similarity between two sentences? Justify the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RQEERDONF9HP",
    "tags": []
   },
   "source": [
    "## Mandatory exercise: Non-probabilistic parsers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "av7OGZFMg7Fj"
   },
   "source": [
    "We add the words `\"lazy\"` (adjective, `Adj`), `\"play\"` (verb, `V`) and `\"with\"` (preposition, `PP`) in order to expand the grammar given so it satisfies the sentence `Lazy cats play with mice`. \n",
    "\n",
    "Reference: https://www.nltk.org/book/ch08.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "xmpGfJcQpfdr"
   },
   "outputs": [],
   "source": [
    "grammar = CFG.fromstring('''\n",
    "  S   -> NP VP\n",
    "  VP  -> V | V PP NP\n",
    "  NP  ->  NNS | JJ NNS\n",
    "  NNS -> \"cats\" | \"mice\" \n",
    "  PP  -> \"with\"\n",
    "  V   -> \"play\"\n",
    "  JJ  -> \"Lazy\"\n",
    "  ''')\n",
    "sent = ['Lazy', 'cats', 'play', 'with', 'mice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns = ['Parser', 'Edges', 'Trees'])\n",
    "def parsers(grammar, sent, parser_func):\n",
    "    parser = getattr(nltk, parser_func)(grammar, trace=1)\n",
    "    parse = parser.parse(sent)\n",
    "    trees = [t for t in parse]\n",
    "    with open(os.devnull, \"w\") as f, contextlib.redirect_stdout(f):\n",
    "        parse = parser.chart_parse(sent)\n",
    "    print('-'*80)\n",
    "    print('Number of trees:', len(trees))\n",
    "    print('Number of edges:', parse.num_edges())\n",
    "    print('-'*80)\n",
    "\n",
    "    [print(edge) for edge in parse.edges()]\n",
    "    return parse.num_edges(), len(trees), trees[0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jFdsYd8uA7G9"
   },
   "source": [
    "### BottomUpChartParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|.  Lazy .  cats .  play .  with .  mice .|\n",
      "|[-------]       .       .       .       .| [0:1] 'Lazy'\n",
      "|.       [-------]       .       .       .| [1:2] 'cats'\n",
      "|.       .       [-------]       .       .| [2:3] 'play'\n",
      "|.       .       .       [-------]       .| [3:4] 'with'\n",
      "|.       .       .       .       [-------]| [4:5] 'mice'\n",
      "|>       .       .       .       .       .| [0:0] JJ -> * 'Lazy'\n",
      "|[-------]       .       .       .       .| [0:1] JJ -> 'Lazy' *\n",
      "|>       .       .       .       .       .| [0:0] NP -> * JJ NNS\n",
      "|[------->       .       .       .       .| [0:1] NP -> JJ * NNS\n",
      "|.       >       .       .       .       .| [1:1] NNS -> * 'cats'\n",
      "|.       [-------]       .       .       .| [1:2] NNS -> 'cats' *\n",
      "|.       >       .       .       .       .| [1:1] NP -> * NNS\n",
      "|[---------------]       .       .       .| [0:2] NP -> JJ NNS *\n",
      "|.       [-------]       .       .       .| [1:2] NP -> NNS *\n",
      "|.       >       .       .       .       .| [1:1] S  -> * NP VP\n",
      "|.       [------->       .       .       .| [1:2] S  -> NP * VP\n",
      "|>       .       .       .       .       .| [0:0] S  -> * NP VP\n",
      "|[--------------->       .       .       .| [0:2] S  -> NP * VP\n",
      "|.       .       >       .       .       .| [2:2] V  -> * 'play'\n",
      "|.       .       [-------]       .       .| [2:3] V  -> 'play' *\n",
      "|.       .       >       .       .       .| [2:2] VP -> * V\n",
      "|.       .       >       .       .       .| [2:2] VP -> * V PP NP\n",
      "|.       .       [-------]       .       .| [2:3] VP -> V *\n",
      "|.       .       [------->       .       .| [2:3] VP -> V * PP NP\n",
      "|.       [---------------]       .       .| [1:3] S  -> NP VP *\n",
      "|[-----------------------]       .       .| [0:3] S  -> NP VP *\n",
      "|.       .       .       >       .       .| [3:3] PP -> * 'with'\n",
      "|.       .       .       [-------]       .| [3:4] PP -> 'with' *\n",
      "|.       .       [--------------->       .| [2:4] VP -> V PP * NP\n",
      "|.       .       .       .       >       .| [4:4] NNS -> * 'mice'\n",
      "|.       .       .       .       [-------]| [4:5] NNS -> 'mice' *\n",
      "|.       .       .       .       >       .| [4:4] NP -> * NNS\n",
      "|.       .       .       .       [-------]| [4:5] NP -> NNS *\n",
      "|.       .       .       .       >       .| [4:4] S  -> * NP VP\n",
      "|.       .       [-----------------------]| [2:5] VP -> V PP NP *\n",
      "|.       .       .       .       [------->| [4:5] S  -> NP * VP\n",
      "|.       [-------------------------------]| [1:5] S  -> NP VP *\n",
      "|[=======================================]| [0:5] S  -> NP VP *\n",
      "--------------------------------------------------------------------------------\n",
      "Number of trees: 1\n",
      "Number of edges: 38\n",
      "--------------------------------------------------------------------------------\n",
      "[0:1] 'Lazy'\n",
      "[1:2] 'cats'\n",
      "[2:3] 'play'\n",
      "[3:4] 'with'\n",
      "[4:5] 'mice'\n",
      "[0:0] JJ -> * 'Lazy'\n",
      "[0:1] JJ -> 'Lazy' *\n",
      "[0:0] NP -> * JJ NNS\n",
      "[0:1] NP -> JJ * NNS\n",
      "[1:1] NNS -> * 'cats'\n",
      "[1:2] NNS -> 'cats' *\n",
      "[1:1] NP -> * NNS\n",
      "[0:2] NP -> JJ NNS *\n",
      "[1:2] NP -> NNS *\n",
      "[1:1] S  -> * NP VP\n",
      "[1:2] S  -> NP * VP\n",
      "[0:0] S  -> * NP VP\n",
      "[0:2] S  -> NP * VP\n",
      "[2:2] V  -> * 'play'\n",
      "[2:3] V  -> 'play' *\n",
      "[2:2] VP -> * V\n",
      "[2:2] VP -> * V PP NP\n",
      "[2:3] VP -> V *\n",
      "[2:3] VP -> V * PP NP\n",
      "[1:3] S  -> NP VP *\n",
      "[0:3] S  -> NP VP *\n",
      "[3:3] PP -> * 'with'\n",
      "[3:4] PP -> 'with' *\n",
      "[2:4] VP -> V PP * NP\n",
      "[4:4] NNS -> * 'mice'\n",
      "[4:5] NNS -> 'mice' *\n",
      "[4:4] NP -> * NNS\n",
      "[4:5] NP -> NNS *\n",
      "[4:4] S  -> * NP VP\n",
      "[2:5] VP -> V PP NP *\n",
      "[4:5] S  -> NP * VP\n",
      "[1:5] S  -> NP VP *\n",
      "[0:5] S  -> NP VP *\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg baseProfile=\"full\" height=\"216px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight:normal; font-style: normal; font-size: 16px;\" version=\"1.1\" viewBox=\"0,0,240.0,216.0\" width=\"240px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">S</text></svg><svg width=\"40%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NP</text></svg><svg width=\"50%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">JJ</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">Lazy</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"25%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"50%\" x=\"50%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNS</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">cats</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"75%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"20%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"60%\" x=\"40%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VP</text></svg><svg width=\"33.3333%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">V</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">play</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"16.6667%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"33.3333%\" x=\"33.3333%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">PP</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">with</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"33.3333%\" x=\"66.6667%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NP</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNS</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">mice</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"83.3333%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"70%\" y1=\"1.2em\" y2=\"3em\" /></svg>"
      ],
      "text/plain": [
       "TreeLayout(Tree('S', [Tree('NP', [Tree('JJ', ['Lazy']), Tree('NNS', ['cats'])]), Tree('VP', [Tree('V', ['play']), Tree('PP', ['with']), Tree('NP', [Tree('NNS', ['mice'])])])]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges, trees, tree = parsers(grammar, sent, 'BottomUpChartParser')\n",
    "results.loc[len(results)] = ['Bottom Up', edges, trees] \n",
    "svgling.draw_tree(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dW1k-WYeDLb8"
   },
   "source": [
    "### BottomUpLeftCornerChartParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|.  Lazy .  cats .  play .  with .  mice .|\n",
      "|[-------]       .       .       .       .| [0:1] 'Lazy'\n",
      "|.       [-------]       .       .       .| [1:2] 'cats'\n",
      "|.       .       [-------]       .       .| [2:3] 'play'\n",
      "|.       .       .       [-------]       .| [3:4] 'with'\n",
      "|.       .       .       .       [-------]| [4:5] 'mice'\n",
      "|[-------]       .       .       .       .| [0:1] JJ -> 'Lazy' *\n",
      "|[------->       .       .       .       .| [0:1] NP -> JJ * NNS\n",
      "|.       [-------]       .       .       .| [1:2] NNS -> 'cats' *\n",
      "|.       [-------]       .       .       .| [1:2] NP -> NNS *\n",
      "|[---------------]       .       .       .| [0:2] NP -> JJ NNS *\n",
      "|[--------------->       .       .       .| [0:2] S  -> NP * VP\n",
      "|.       [------->       .       .       .| [1:2] S  -> NP * VP\n",
      "|.       .       [-------]       .       .| [2:3] V  -> 'play' *\n",
      "|.       .       [-------]       .       .| [2:3] VP -> V *\n",
      "|.       .       [------->       .       .| [2:3] VP -> V * PP NP\n",
      "|[-----------------------]       .       .| [0:3] S  -> NP VP *\n",
      "|.       [---------------]       .       .| [1:3] S  -> NP VP *\n",
      "|.       .       .       [-------]       .| [3:4] PP -> 'with' *\n",
      "|.       .       [--------------->       .| [2:4] VP -> V PP * NP\n",
      "|.       .       .       .       [-------]| [4:5] NNS -> 'mice' *\n",
      "|.       .       .       .       [-------]| [4:5] NP -> NNS *\n",
      "|.       .       .       .       [------->| [4:5] S  -> NP * VP\n",
      "|.       .       [-----------------------]| [2:5] VP -> V PP NP *\n",
      "|[=======================================]| [0:5] S  -> NP VP *\n",
      "|.       [-------------------------------]| [1:5] S  -> NP VP *\n",
      "--------------------------------------------------------------------------------\n",
      "Number of trees: 1\n",
      "Number of edges: 25\n",
      "--------------------------------------------------------------------------------\n",
      "[0:1] 'Lazy'\n",
      "[1:2] 'cats'\n",
      "[2:3] 'play'\n",
      "[3:4] 'with'\n",
      "[4:5] 'mice'\n",
      "[0:1] JJ -> 'Lazy' *\n",
      "[0:1] NP -> JJ * NNS\n",
      "[1:2] NNS -> 'cats' *\n",
      "[1:2] NP -> NNS *\n",
      "[0:2] NP -> JJ NNS *\n",
      "[0:2] S  -> NP * VP\n",
      "[1:2] S  -> NP * VP\n",
      "[2:3] V  -> 'play' *\n",
      "[2:3] VP -> V *\n",
      "[2:3] VP -> V * PP NP\n",
      "[0:3] S  -> NP VP *\n",
      "[1:3] S  -> NP VP *\n",
      "[3:4] PP -> 'with' *\n",
      "[2:4] VP -> V PP * NP\n",
      "[4:5] NNS -> 'mice' *\n",
      "[4:5] NP -> NNS *\n",
      "[4:5] S  -> NP * VP\n",
      "[2:5] VP -> V PP NP *\n",
      "[0:5] S  -> NP VP *\n",
      "[1:5] S  -> NP VP *\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg baseProfile=\"full\" height=\"216px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight:normal; font-style: normal; font-size: 16px;\" version=\"1.1\" viewBox=\"0,0,240.0,216.0\" width=\"240px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">S</text></svg><svg width=\"40%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NP</text></svg><svg width=\"50%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">JJ</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">Lazy</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"25%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"50%\" x=\"50%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNS</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">cats</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"75%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"20%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"60%\" x=\"40%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VP</text></svg><svg width=\"33.3333%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">V</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">play</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"16.6667%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"33.3333%\" x=\"33.3333%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">PP</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">with</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"33.3333%\" x=\"66.6667%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NP</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNS</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">mice</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"83.3333%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"70%\" y1=\"1.2em\" y2=\"3em\" /></svg>"
      ],
      "text/plain": [
       "TreeLayout(Tree('S', [Tree('NP', [Tree('JJ', ['Lazy']), Tree('NNS', ['cats'])]), Tree('VP', [Tree('V', ['play']), Tree('PP', ['with']), Tree('NP', [Tree('NNS', ['mice'])])])]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges, trees, tree = parsers(grammar, sent, 'BottomUpLeftCornerChartParser')\n",
    "results.loc[len(results)] = ['Bottom Up Left Corner', edges, trees] \n",
    "svgling.draw_tree(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qOOmX_GXDWIz"
   },
   "source": [
    "### LeftCornerChartParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|.  Lazy .  cats .  play .  with .  mice .|\n",
      "|[-------]       .       .       .       .| [0:1] 'Lazy'\n",
      "|.       [-------]       .       .       .| [1:2] 'cats'\n",
      "|.       .       [-------]       .       .| [2:3] 'play'\n",
      "|.       .       .       [-------]       .| [3:4] 'with'\n",
      "|.       .       .       .       [-------]| [4:5] 'mice'\n",
      "|[-------]       .       .       .       .| [0:1] JJ -> 'Lazy' *\n",
      "|[------->       .       .       .       .| [0:1] NP -> JJ * NNS\n",
      "|.       [-------]       .       .       .| [1:2] NNS -> 'cats' *\n",
      "|.       [-------]       .       .       .| [1:2] NP -> NNS *\n",
      "|[---------------]       .       .       .| [0:2] NP -> JJ NNS *\n",
      "|[--------------->       .       .       .| [0:2] S  -> NP * VP\n",
      "|.       [------->       .       .       .| [1:2] S  -> NP * VP\n",
      "|.       .       [-------]       .       .| [2:3] V  -> 'play' *\n",
      "|.       .       [-------]       .       .| [2:3] VP -> V *\n",
      "|.       .       [------->       .       .| [2:3] VP -> V * PP NP\n",
      "|[-----------------------]       .       .| [0:3] S  -> NP VP *\n",
      "|.       [---------------]       .       .| [1:3] S  -> NP VP *\n",
      "|.       .       .       [-------]       .| [3:4] PP -> 'with' *\n",
      "|.       .       [--------------->       .| [2:4] VP -> V PP * NP\n",
      "|.       .       .       .       [-------]| [4:5] NNS -> 'mice' *\n",
      "|.       .       .       .       [-------]| [4:5] NP -> NNS *\n",
      "|.       .       [-----------------------]| [2:5] VP -> V PP NP *\n",
      "|[=======================================]| [0:5] S  -> NP VP *\n",
      "|.       [-------------------------------]| [1:5] S  -> NP VP *\n",
      "--------------------------------------------------------------------------------\n",
      "Number of trees: 1\n",
      "Number of edges: 24\n",
      "--------------------------------------------------------------------------------\n",
      "[0:1] 'Lazy'\n",
      "[1:2] 'cats'\n",
      "[2:3] 'play'\n",
      "[3:4] 'with'\n",
      "[4:5] 'mice'\n",
      "[0:1] JJ -> 'Lazy' *\n",
      "[0:1] NP -> JJ * NNS\n",
      "[1:2] NNS -> 'cats' *\n",
      "[1:2] NP -> NNS *\n",
      "[0:2] NP -> JJ NNS *\n",
      "[0:2] S  -> NP * VP\n",
      "[1:2] S  -> NP * VP\n",
      "[2:3] V  -> 'play' *\n",
      "[2:3] VP -> V *\n",
      "[2:3] VP -> V * PP NP\n",
      "[0:3] S  -> NP VP *\n",
      "[1:3] S  -> NP VP *\n",
      "[3:4] PP -> 'with' *\n",
      "[2:4] VP -> V PP * NP\n",
      "[4:5] NNS -> 'mice' *\n",
      "[4:5] NP -> NNS *\n",
      "[2:5] VP -> V PP NP *\n",
      "[0:5] S  -> NP VP *\n",
      "[1:5] S  -> NP VP *\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg baseProfile=\"full\" height=\"216px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight:normal; font-style: normal; font-size: 16px;\" version=\"1.1\" viewBox=\"0,0,240.0,216.0\" width=\"240px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">S</text></svg><svg width=\"40%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NP</text></svg><svg width=\"50%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">JJ</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">Lazy</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"25%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"50%\" x=\"50%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNS</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">cats</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"75%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"20%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"60%\" x=\"40%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VP</text></svg><svg width=\"33.3333%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">V</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">play</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"16.6667%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"33.3333%\" x=\"33.3333%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">PP</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">with</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"33.3333%\" x=\"66.6667%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NP</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNS</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">mice</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"83.3333%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"70%\" y1=\"1.2em\" y2=\"3em\" /></svg>"
      ],
      "text/plain": [
       "TreeLayout(Tree('S', [Tree('NP', [Tree('JJ', ['Lazy']), Tree('NNS', ['cats'])]), Tree('VP', [Tree('V', ['play']), Tree('PP', ['with']), Tree('NP', [Tree('NNS', ['mice'])])])]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges, trees, tree = parsers(grammar, sent, 'LeftCornerChartParser')\n",
    "results.loc[len(results)] = ['Left Corner', edges, trees] \n",
    "svgling.draw_tree(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LUJ7w1FuGNb1"
   },
   "source": [
    "### **Conclusions**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Parser</th>\n",
       "      <th>Edges</th>\n",
       "      <th>Trees</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bottom Up</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bottom Up Left Corner</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Left Corner</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Parser  Edges  Trees\n",
       "0              Bottom Up     38      1\n",
       "1  Bottom Up Left Corner     25      1\n",
       "2            Left Corner     24      1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Which parser is the most efficient for parsing the sentence?**\n",
    "\n",
    "We have seen that all three parsers correctly subsum the sentence. The main difference, in this case, is in the number of edges the parser needs to return the final tree. The number of edges basically means the efficiency of the parser in returning the tree, where fewer edges means more efficiency, i.e. fewer edges traversed to complete the parse.\n",
    "\n",
    "We see in the summary table, that the Bottom Up parser returns the worst results with 38 edges, while the Bottom Left Corner and the Left Corner return almost identical results, with the Left Corner beating the Bottom Up Left Corner by one edge.\n",
    "\n",
    "\n",
    "**Which edges are filtered out by each parser and why?**\n",
    "\n",
    "https://www.cs.upc.edu/~gatius/mai-ihlp/session9.pdf\n",
    "\n",
    "Comparing the Bottom Up, with the Bottom Up Left Corner, we see that the Bottom up has a few more edges. For example, it always has the `*` on the left first. These have been removed on the Bottom Up Left Corner.\n",
    "\n",
    "- **Bottom-Up**: it takes the input string and tries to combine words to constituents and constituents to bigger constituents using the grammar rules from right to left. In doing so, any constituent that can be built are built; no matter whether they fit into the constituent that we are working on a the moment or not. \n",
    "\n",
    "\n",
    "- **Bottom-up Corner**: It filters out the edges without any word subsumtion $X \\rightarrow . Y Z$.\n",
    "\n",
    "- **Left-Corner**: alternates steps of bottom-up processing with top-down predictions. It imposes top-down constraints that what follow in what the following input string can be. It starts with top-down prediction fixing the category that is to be recognized and then takes a bottom-up step and alternates between the both until it is subsumed. It filters out edges without new word subsumptions, If we have $Y\\rightarrow y$ and $Z \\rightarrow z$ then we filter $A \\rightarrow Y.Z$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "For example, on the Bottom Up, the following appears:\n",
    "```\n",
    "[0:0] JJ -> * 'Lazy'\n",
    "[0:0] NP -> * JJ NNS\n",
    "[1:1] NNS -> * 'cats'\n",
    "[1:1] NP -> * NNS\n",
    "[1:1] S  -> * NP VP\n",
    "[0:0] S  -> * NP VP\n",
    "[2:2] V  -> * 'play'\n",
    "[2:2] VP -> * V\n",
    "[2:2] VP -> * V PP NP\n",
    "[4:4] NP -> * NNS\n",
    "[2:2] VP -> * V\n",
    "[3:3] PP -> * 'with'\n",
    "[4:4] NNS -> * 'mice'\n",
    "```\n",
    "\n",
    "Every edge starts with the `* constituent` and this is filtered on the Bottom Up Left Corner and Left Corner. Thanks to the left corner filtering.\n",
    "\n",
    "\n",
    "Comparing Bottom Up Left Corner with Left Corner, we can see that the Left Corner does not have the edge `[4:5] S  -> NP * VP`, which the bottom up left corner does have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1txa_-0hGB1h",
    "tags": []
   },
   "source": [
    "## Optional exercise: Dependency parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6mJzm_UqKoar"
   },
   "source": [
    "To use, we first need to download and run the CoreNLP server on `localhost:9000` by following the next few steps:\n",
    "\n",
    "1. Download CoreNLP at https://stanfordnlp.github.io/CoreNLP/download.html\n",
    "2. Unzip the files and run the following command in the that directory to start the server: `java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000`\n",
    "\n",
    "`java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -preload tokenize,ssplit,pos,lemma,ner,parse,depparse -status_port 9000 -port 9000 -timeout 15000 & `\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "id": "0pw1gRZaJur1",
    "outputId": "d675e2be-5ff2-48cf-d570-d2748d76dd3c"
   },
   "outputs": [],
   "source": [
    "parser = CoreNLPDependencyParser(url='http://localhost:9000/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_jaccard_distance(sentence1, sentence2):\n",
    "    if len(sentence1.union(sentence2)) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 5*(1 - jaccard_distance(sentence1, sentence2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_reader(function_preprocess):\n",
    "    dt = pd.read_csv(path + 'STS.input.SMTeuroparl.txt', sep='\\t', header = None)\n",
    "    dt[2] = dt.apply(lambda row: function_preprocess(row[0]), axis = 1)\n",
    "    dt[3] = dt.apply(lambda row: function_preprocess(row[1]), axis = 1)\n",
    "    dt['gs'] = pd.read_csv(path + 'STS.gs.SMTeuroparl.txt', sep='\\t', header = None)\n",
    "    dt['jac'] = dt.apply(lambda row: apply_jaccard_distance(row[2], row[3]), axis = 1)\n",
    "    return dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/home/benja/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nltk/corpus/util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mzip_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - '/home/benja/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m stopw \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorpus\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstopwords\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwords\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;66;03m# english stopwords\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_CoreNLDPependencyParser\u001b[39m(sentence):\n\u001b[1;32m      5\u001b[0m     parse, \u001b[38;5;241m=\u001b[39m parser\u001b[38;5;241m.\u001b[39mraw_parse(sentence)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nltk/corpus/util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nltk/corpus/util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[0;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nltk/corpus/util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/home/benja/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "stopw = set(nltk.corpus.stopwords.words('english')) # english stopwords\n",
    "\n",
    "\n",
    "def apply_CoreNLDPependencyParser(sentence):\n",
    "    parse, = parser.raw_parse(sentence)\n",
    "    triples = []\n",
    "    for governor, dep, dependent in parse.triples():\n",
    "        if dep == 'punct' or governor[0].lower() in stopw:\n",
    "            continue\n",
    "        triples.append(( (governor[0].lower(), governor[1]), dep, (dependent[0].lower(), dependent[1])))\n",
    "    return set(triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = data_reader(apply_CoreNLDPependencyParser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "styler = dt.head(3).style.set_table_attributes(\"style='display:inline'\")\n",
    "display_html(styler._repr_html_(), raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearsonr(dt['gs'], dt['jac'])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Is it relevant to compute the triples?**\n",
    "\n",
    "Comparing the triples like we are doing now is not relevant for the STS task. CoreNLP takes in a raw test and runs a series of steps (tokenize, split, POS, lemma, ner, deparse) to obtain a final set of annotations. Using the POS tags of these sets might be better than using the previously tested ones. However, using the entire set of triples to compare the STS results in worse results than just comparing the tokens. Hence, we might use it to extract the POS and perhaps it could improve the default NLTK results."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
