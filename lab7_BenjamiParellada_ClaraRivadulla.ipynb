{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"HxH2ZDs1HstB"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# path where the data is contained\n","path = '/content/drive/MyDrive/IHLT/lab2/'\n","\n","import pandas as pd\n","import re\n","import nltk\n","from nltk import ne_chunk\n","from nltk.metrics import jaccard_distance\n","from nltk.corpus import wordnet\n","from nltk.stem import WordNetLemmatizer\n","from scipy.stats import pearsonr\n","from IPython.display import display_html \n","\n","\n","import nltk\n","nltk.download('maxent_ne_chunker')\n","nltk.download('conll2000')\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('words')"]},{"cell_type":"code","source":["# read the dataset and apply the lesk function for each pair of sentences\n","def data_reader(function_preprocess):\n","  dt = pd.read_csv(path + 'STS.input.SMTeuroparl.txt', sep='\\t', header = None) # i guess we could read this just once but eh\n","  dt[2] = dt.apply(lambda row: function_preprocess(row[0]), axis = 1)\n","  dt[3] = dt.apply(lambda row: function_preprocess(row[1]), axis = 1)\n","  dt['gs'] = pd.read_csv(path + 'STS.gs.SMTeuroparl.txt', sep='\\t', header = None)\n","  dt['jac'] = dt.apply(lambda row: 5*(1 - jaccard_distance(row[2], row[3])), axis = 1)\n","  #dt.drop_duplicates(subset = [0, 1], keep=False, inplace = True) # drop duplicate rows, but no one seems to be doing it so why should we \n","  return dt"],"metadata":{"id":"BsjbT5VtMFZ5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Lab 7: Word Sequences\n","\n","For the seventh practical of the subject, the goal is to get in touch with Word Sequences and ust NERC models. The statement is as follows:\n","\n","1. Read all pairs of sentences of the SMTeuroparl files of test set within the evaluation framework of the project.\n","2. Compute their Jaccard similarities by considering the words plus the named entities.\n","3. Show the results and answer: \"Do you think it could be relevant to use NEs to compute the similarity between two sentences?\" \n","4. Optional exercise: given a grammar enlarge it to parse it as they say\n","\n"],"metadata":{"id":"AuqgGsE6Lcjx"}},{"cell_type":"code","source":["stopw = set(nltk.corpus.stopwords.words('english')) # english stopwords\n","\n","def NE_basic(sentence):\n","  tokens = nltk.word_tokenize(sentence) # tokenize\n","  pairs = nltk.pos_tag(tokens) # get the pos of the tokens\n","  chunks = nltk.ne_chunk(pairs, binary = False) # chunk it # https://www.youtube.com/watch?v=zDnPFxnALBg\n","  triads = nltk.tree2conlltags(chunks) # get the triads with the token, pos tag, and if its named entity\n","  text = []\n","  for triad in triads:\n","    token = re.sub(r'[^\\w\\s]', '', triad[0]).lower() # remove punctuaction inside each token\n","    if token in stopw or re.match(r'^[_\\W]+$', token) or token == '': # if its stopword, non-alphanumeric token, or empty we skip\n","      continue\n","    if triad[2][0] == 'O': # if not named enitity we append to text\n","      text.append(token)\n","    elif triad[2][0] == 'B': # if named entity we append to text as well\n","      text.append(token)\n","    else: # if the named entity continues, we concatanate it with the last string\n","      text[-1] += ' ' + token\n","  return set(text)"],"metadata":{"id":"XMGWtMF3IhiI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dt = data_reader(NE_basic)"],"metadata":{"id":"xTIzLVO2P_Ke"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let us check some concrete examples where the NE worked. In the following output, we can see how `European` and `Union` got correctly joined to form `European Union`. While this makes sense in a human standpoint, in the case of our score, it actually returns a worse result. This is due that we are only checking the intersection of the elements, hence, since we have one less word that will be matched, we get a worse result. Since we now only have one word in the set that matches, the score is $5\\frac{1}{4 + 5 - 1} = 0.625$, while if we had the set with `european` and `union` separated we would get $5\\frac{2}{5 + 6 - 2} = 1.11$, which is closer to the gold standard.\n","\n","Additionally, on the first sentence pair, we can see that since `report` appears in uppercase in the first sentence and lowercase on the second, it will change the named entity detection. This little change makes it not be detected as a named entity on the second pair. Observing the chunking output:\n","\n","1. `('Van', 'NNP', 'B-PERSON'), ('Orden', 'NNP', 'B-PERSON'), ('Report', 'NNP', 'I-PERSON')`\n","\n","2. `('Van', 'NNP', 'B-PERSON'), ('Orden', 'NNP', 'B-ORGANIZATION'), ('report', 'NN', 'O')`\n","\n","Moreover, we can see how `Van` and `Orden` are detected both as beginnings of NE in both sentences. Hence, this is the reason we do not join `Van` with `Orden Report`.\n","\n","Finally, the Pearson correlation returns lower than the second practical."],"metadata":{"id":"E8tEDlFrHy_g"}},{"cell_type":"code","source":["styler = dt.iloc[[373, 374]].style.set_table_attributes(\"style='display:inline'\")\n","display_html(styler._repr_html_(), raw=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":83},"id":"ksmc-we0FF2A","executionInfo":{"status":"ok","timestamp":1667984815804,"user_tz":-60,"elapsed":454,"user":{"displayName":"Clara Rivadulla Duró","userId":"06370667205959195966"}},"outputId":"3f70a98b-a4a2-4260-bc13-64013bb64486"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<style type=\"text/css\">\n","</style>\n","<table id=\"T_1219d_\" style='display:inline'>\n","  <thead>\n","    <tr>\n","      <th class=\"blank level0\" >&nbsp;</th>\n","      <th class=\"col_heading level0 col0\" >0</th>\n","      <th class=\"col_heading level0 col1\" >1</th>\n","      <th class=\"col_heading level0 col2\" >2</th>\n","      <th class=\"col_heading level0 col3\" >3</th>\n","      <th class=\"col_heading level0 col4\" >gs</th>\n","      <th class=\"col_heading level0 col5\" >jac</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th id=\"T_1219d_level0_row0\" class=\"row_heading level0 row0\" >373</th>\n","      <td id=\"T_1219d_row0_col0\" class=\"data row0 col0\" >Van Orden Report (A5-0241/2000)</td>\n","      <td id=\"T_1219d_row0_col1\" class=\"data row0 col1\" >Van Orden report (A5-0241 / 2000)</td>\n","      <td id=\"T_1219d_row0_col2\" class=\"data row0 col2\" >{'van', 'a502412000', 'orden report'}</td>\n","      <td id=\"T_1219d_row0_col3\" class=\"data row0 col3\" >{'a50241', '2000', 'orden', 'van', 'report'}</td>\n","      <td id=\"T_1219d_row0_col4\" class=\"data row0 col4\" >5.000000</td>\n","      <td id=\"T_1219d_row0_col5\" class=\"data row0 col5\" >0.714286</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_1219d_level0_row1\" class=\"row_heading level0 row1\" >374</th>\n","      <td id=\"T_1219d_row1_col0\" class=\"data row1 col0\" >The European Union has got to do something and do it quickly.</td>\n","      <td id=\"T_1219d_row1_col1\" class=\"data row1 col1\" >It suits that the European Union is implied and that it makesit rapidly.</td>\n","      <td id=\"T_1219d_row1_col2\" class=\"data row1 col2\" >{'quickly', 'something', 'european union', 'got'}</td>\n","      <td id=\"T_1219d_row1_col3\" class=\"data row1 col3\" >{'makesit', 'suits', 'european union', 'implied', 'rapidly'}</td>\n","      <td id=\"T_1219d_row1_col4\" class=\"data row1 col4\" >3.000000</td>\n","      <td id=\"T_1219d_row1_col5\" class=\"data row1 col5\" >0.625000</td>\n","    </tr>\n","  </tbody>\n","</table>\n"]},"metadata":{}}]},{"cell_type":"code","source":["pearsonr(dt['gs'], dt['jac'])[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0htxELqTQ9LU","executionInfo":{"status":"ok","timestamp":1667984815805,"user_tz":-60,"elapsed":10,"user":{"displayName":"Clara Rivadulla Duró","userId":"06370667205959195966"}},"outputId":"1db40252-df31-4fe8-ac36-4c03dd5fc6c0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.4479641166806418"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","source":["## With Spacy"],"metadata":{"id":"OqPbKq9OO7vy"}},{"cell_type":"code","source":["import spacy\n","nlp = spacy.load(\"en_core_web_sm\")"],"metadata":{"id":"18qgtLNLJpvk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def NE_spacy(sentence, print = False):\n","  tokens = nlp(sentence) # why is spacy so much simpler \n","  text = []\n","  if print:\n","    spacy.displacy.render(tokens, jupyter = True, style='ent')\n","  with tokens.retokenize() as retokenizer:\n","    token = [t for t in tokens]\n","    for ent in tokens.ents:\n","        retokenizer.merge(tokens[ent.start:ent.end], \n","                          attrs={\"LEMMA\": \" \".join([tokens[i].text for i in range(ent.start, ent.end)])})\n","  \n","  text = []\n","  for token in tokens:\n","    token = re.sub(r'[^\\w\\s]', '', token.text).lower() # remove punctuaction inside each token\n","    if token in stopw or re.match(r'^[_\\W]+$', token) or token == '': # if its stopword, non-alphanumeric token, or empty we skip\n","      continue\n","    text.append(token)\n","  return set(text)"],"metadata":{"id":"MwQxawxnJsl6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dt = data_reader(NE_spacy)"],"metadata":{"id":"Q-MPIBr9OI7c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["styler = dt.iloc[[373, 374]].style.set_table_attributes(\"style='display:inline'\")\n","display_html(styler._repr_html_(), raw=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":83},"id":"9gBLMeAWOO-8","executionInfo":{"status":"ok","timestamp":1667984830379,"user_tz":-60,"elapsed":16,"user":{"displayName":"Clara Rivadulla Duró","userId":"06370667205959195966"}},"outputId":"54c65412-69e7-48b8-dd6c-2736aa1b17b6"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<style type=\"text/css\">\n","</style>\n","<table id=\"T_2579d_\" style='display:inline'>\n","  <thead>\n","    <tr>\n","      <th class=\"blank level0\" >&nbsp;</th>\n","      <th class=\"col_heading level0 col0\" >0</th>\n","      <th class=\"col_heading level0 col1\" >1</th>\n","      <th class=\"col_heading level0 col2\" >2</th>\n","      <th class=\"col_heading level0 col3\" >3</th>\n","      <th class=\"col_heading level0 col4\" >gs</th>\n","      <th class=\"col_heading level0 col5\" >jac</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th id=\"T_2579d_level0_row0\" class=\"row_heading level0 row0\" >373</th>\n","      <td id=\"T_2579d_row0_col0\" class=\"data row0 col0\" >Van Orden Report (A5-0241/2000)</td>\n","      <td id=\"T_2579d_row0_col1\" class=\"data row0 col1\" >Van Orden report (A5-0241 / 2000)</td>\n","      <td id=\"T_2579d_row0_col2\" class=\"data row0 col2\" >{'a502412000', 'van orden report'}</td>\n","      <td id=\"T_2579d_row0_col3\" class=\"data row0 col3\" >{'report', 'a50241  2000', 'van orden'}</td>\n","      <td id=\"T_2579d_row0_col4\" class=\"data row0 col4\" >5.000000</td>\n","      <td id=\"T_2579d_row0_col5\" class=\"data row0 col5\" >0.000000</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_2579d_level0_row1\" class=\"row_heading level0 row1\" >374</th>\n","      <td id=\"T_2579d_row1_col0\" class=\"data row1 col0\" >The European Union has got to do something and do it quickly.</td>\n","      <td id=\"T_2579d_row1_col1\" class=\"data row1 col1\" >It suits that the European Union is implied and that it makesit rapidly.</td>\n","      <td id=\"T_2579d_row1_col2\" class=\"data row1 col2\" >{'quickly', 'the european union', 'got', 'something'}</td>\n","      <td id=\"T_2579d_row1_col3\" class=\"data row1 col3\" >{'makesit', 'suits', 'the european union', 'implied', 'rapidly'}</td>\n","      <td id=\"T_2579d_row1_col4\" class=\"data row1 col4\" >3.000000</td>\n","      <td id=\"T_2579d_row1_col5\" class=\"data row1 col5\" >0.625000</td>\n","    </tr>\n","  </tbody>\n","</table>\n"]},"metadata":{}}]},{"cell_type":"code","source":["print('First sentence')\n","_ = dt.iloc[[373, 374]].apply(lambda row: NE_spacy(row[0], True), axis = 1)\n","print('Second sentence')\n","_ = dt.iloc[[373, 374]].apply(lambda row: NE_spacy(row[1], True), axis = 1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":193},"id":"Eth3PURKvwmp","executionInfo":{"status":"ok","timestamp":1667984830379,"user_tz":-60,"elapsed":13,"user":{"displayName":"Clara Rivadulla Duró","userId":"06370667205959195966"}},"outputId":"937c0404-fec0-4699-d0e4-42e5c06d34a4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["First sentence\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n","<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    Van Orden Report\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n","</mark>\n"," (\n","<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    A5-0241/2000\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n","</mark>\n",")</div></span>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n","<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    The European Union\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n","</mark>\n"," has got to do something and do it quickly.</div></span>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Second sentence\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n","<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    Van Orden\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n","</mark>\n"," report (\n","<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    A5-0241 / 2000\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n","</mark>\n",")</div></span>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">It suits that \n","<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    the European Union\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n","</mark>\n"," is implied and that it makesit rapidly.</div></span>"]},"metadata":{}}]},{"cell_type":"code","source":["pearsonr(dt['gs'], dt['jac'])[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"enax2jDbOQLj","executionInfo":{"status":"ok","timestamp":1667984830380,"user_tz":-60,"elapsed":11,"user":{"displayName":"Clara Rivadulla Duró","userId":"06370667205959195966"}},"outputId":"e30ea51f-2f95-41c6-fb99-06c6542951b5"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.4399393862466662"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","source":["We do not obtain much better correlation, and we can see how now it joined even more words. Which can be good or bad, and it seems worse for the paraphrase detection. Moreover, since we have not removed the stopwords, it joined `The` with `European Union`, which we could probably avoid if we first remove stopwords. However, if we first remove stopwords the POS tags might not work properly, and the chunking might not work properly, since they might depend on them to correctly infer the context.\n","\n","Moreover, we see that `a50241/2000` got detected as different things, cardinal or date depending on if there is a space in between the `/` or not. However, our match fails since we removed all the symbols inside tokens, and there appears a space on the second pair due to the fact that it joined `a50241` and `2000`. We could fix this by removing all white spaces inside joined tokens, but then all NE would be joined and look ugly. Additionally, while it might work well in this case, there might be some edge case where it does not as expected. We will have to deal with these problems for the last practical, this was more to see how Spacy worked than have a perfect implementation.\n","\n"],"metadata":{"id":"KRlYFWbdO_8u"}},{"cell_type":"markdown","source":["# Conclusion\n","\n","\n","Comparing the results with the simple preprocessing we did in Lab 2 (not taking into consideration the NE, removing stopwords, symbols, etc.), we can see that we obtain a worse result than just using the tokens.\n","\n","We can attribute this to two different reasons. First, using NE, we will join words together, and in some cases the resulting positive matches from the intersection will be lower than having them seperate and thus, returns a lower score, which we have seen in the `European Union` case. Secondly, the concatenation of these words is not working perfectly, as we have seen in the `Van Ordern Report` case. Which the three words sometimes are joined into different subsets, depending on if `report` is upper or lowercase. Moreover, Spacy actually joined on both sentences `Van Orden`, where NLTK never joined `Van` with `Orden`. And in the pair of sentences observed, the intersection match will actually fail, while having them seperate will not.\n","\n","\n","| Processing | Pearson $r$ |  \n","|------------|:---:|\n","| Simple (Lab 2)    | 0.468 |\n","| **Simple + NE** | **0.447**  |\n","\n","**Do you think it could be relevant to use NEs to compute the similarity between two sentences?** \n","\n","Nevertheless, we think it is more appropriate to use the NE as it makes sense to consider certain words together, like \"European Union\". However, it seems to not work as well as expected due to difference in upper/lower case, which could be fixed while preprocessing. Moreover, since we are only checking if two strings appear exactly the same in the intersection of sets, this could make us fail the intersection match due to words being outside the joined string. Additionally, if the NE joined strings match, it usually means that the separate strings would also match, and in some cases this would return a score closer to the gold standard. Hence, we would need to do a smarter matching system, which would probably improve our performance. An idea would be to weigh the length of the string in order to score NE matches higher.\n","\n","In conclusion, we think that using NE makes sense, but it should be used with more preprocessing or a smarter intersection comparison."],"metadata":{"id":"ydalS3MLIoJb"}},{"cell_type":"markdown","source":["# Extra Exercise"],"metadata":{"id":"JliaV_xYPT5C"}},{"cell_type":"code","source":["#!pip install svgling\n","import svgling\n","sentence = [(\"the\", \"DT\"), (\"little\", \"JJ\"), (\"yellow\", \"JJ\"),(\"dog\", \"NN\"),\\\n","          (\"barked\", \"VBD\"), (\"at\", \"IN\"), (\"the\", \"DT\"), (\"cat\", \"NN\"), \\\n","          (\"in\", \"IN\"), (\"New\", \"NNP\"), (\"York\", \"NNP\")]\n","\n","grammar = \\\n","r\"\"\"\n","  NP: {<DT>?<JJ>*<NN.*>+}      # Chunk sequences of DT, JJ, NN as well as NN.*\n","  PP: {<IN><NP>}               # Chunk prepositions followed by NP\n","\"\"\"\n","\n","cp = nltk.RegexpParser(grammar)\n","result = cp.parse(sentence)\n","svgling.draw_tree(result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":237},"id":"FQceHF1EPVIU","executionInfo":{"status":"ok","timestamp":1667984929802,"user_tz":-60,"elapsed":330,"user":{"displayName":"Clara Rivadulla Duró","userId":"06370667205959195966"}},"outputId":"f58249cb-b8ce-4a7c-edde-25c6c2aed40c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TreeLayout(Tree('S', [Tree('NP', [('the', 'DT'), ('little', 'JJ'), ('yellow', 'JJ'), ('dog', 'NN')]), ('barked', 'VBD'), Tree('PP', [('at', 'IN'), Tree('NP', [('the', 'DT'), ('cat', 'NN')])]), Tree('PP', [('in', 'IN'), Tree('NP', [('New', 'NNP'), ('York', 'NNP')])])]))"],"image/svg+xml":"<svg baseProfile=\"full\" height=\"216px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight:normal; font-style: normal; font-size: 16px;\" version=\"1.1\" viewBox=\"0,0,504.0,216.0\" width=\"504px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">S</text></svg><svg width=\"41.2698%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NP</text></svg><svg width=\"19.2308%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">the</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"9.61538%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"30.7692%\" x=\"19.2308%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">little</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">JJ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"34.6154%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"30.7692%\" x=\"50%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">yellow</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">JJ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"65.3846%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"19.2308%\" x=\"80.7692%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">dog</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"90.3846%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"20.6349%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"12.6984%\" x=\"41.2698%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">barked</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VBD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"47.619%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"22.2222%\" x=\"53.9683%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">PP</text></svg><svg width=\"28.5714%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">at</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"14.2857%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"71.4286%\" x=\"28.5714%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NP</text></svg><svg width=\"50%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">the</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"25%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"50%\" x=\"50%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">cat</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"75%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"64.2857%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"65.0794%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"23.8095%\" x=\"76.1905%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">PP</text></svg><svg width=\"26.6667%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">in</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"13.3333%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"73.3333%\" x=\"26.6667%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NP</text></svg><svg width=\"45.4545%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">New</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"22.7273%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"54.5455%\" x=\"45.4545%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">York</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"72.7273%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"63.3333%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"88.0952%\" y1=\"1.2em\" y2=\"3em\" /></svg>"},"metadata":{},"execution_count":15}]}]}